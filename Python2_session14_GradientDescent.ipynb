{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[-0.83366657  1.29278843 -2.56597971]\n",
      " [ 0.31213224 -0.713128    1.2249785 ]\n",
      " [-0.85575908 -1.53145733 -0.09201719]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "random = np.random.randn(3, 3) # 3x3 matrix of random values (normal distribution)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [ 7.44888617e+00  8.51439109e+00  4.76819250e+00  7.00848738e+00\n",
      "  9.59302865e+00  7.89170486e+00  1.03254880e+01  2.17497640e+00\n",
      "  1.79651512e+00  7.41609083e+00 -2.70663271e+00 -1.82559453e+00\n",
      "  1.02842936e+01  3.06614186e+00  3.60478338e+00  6.75823062e+00\n",
      "  1.23440031e+01  7.51657687e+00  2.54369234e+00  4.68547093e+00\n",
      "  3.03195548e+00  1.79810642e+00  1.88606865e+00  4.75193072e+00\n",
      " -4.47752253e+00  5.89062924e+00  1.18105049e+01 -2.35809341e-01\n",
      "  6.51226562e+00  8.28156408e+00  7.50817991e-01  3.52178182e+00\n",
      "  4.68621729e+00  8.85365535e+00  6.88061284e+00 -5.31682221e-01\n",
      "  2.89750466e+00  8.43102968e+00  3.50779472e+00 -2.09336377e-01\n",
      "  1.18691115e+01  3.42798148e+00  5.15044645e+00  9.52321516e+00\n",
      " -1.61317286e-01  1.03581631e+01  1.12762494e+01  1.35174276e+01\n",
      "  9.27959471e-01  9.77768304e-01  3.26837989e+00  3.36722661e+00\n",
      "  5.28617855e+00  1.09051200e+01  1.16346435e+01  5.79354077e+00\n",
      "  5.81302870e+00  4.14996338e+00  2.67774306e+00  1.29136208e+01\n",
      "  7.64504740e+00  7.81806532e+00  4.93118587e+00  4.81770607e+00\n",
      "  1.00480446e+01  9.09472345e+00  9.65349452e+00  3.91159278e+00\n",
      "  6.29960180e+00  1.00040022e+01  3.59830393e+00  8.54467869e+00\n",
      "  3.55516937e+00 -5.73845449e+00  1.13091281e+01  5.33630396e+00\n",
      "  9.52505126e-02  4.29601522e+00  4.03145772e+00  8.96781126e+00\n",
      "  4.57401699e+00  1.04307960e+01  8.98494305e+00  1.15998502e+01\n",
      "  3.35162903e+00 -1.46416137e+00  1.78602396e+00  1.46888035e+01\n",
      "  1.13107218e+01  3.87598381e+00  1.46528097e+01  9.14890161e+00\n",
      "  7.60358159e+00  4.82525811e+00  1.57039379e+01  2.50954804e+00\n",
      "  7.42924868e+00  7.35117793e+00  9.48861437e+00  3.71402489e+00\n",
      "  6.42358403e+00  6.46988718e+00  1.04464757e+01  7.00566418e+00\n",
      "  5.76176718e+00  2.18977215e+00  6.24932734e+00  1.21926674e+01\n",
      "  8.97479169e+00  5.34142429e+00  8.22960359e+00  1.21714378e-02\n",
      "  5.09325323e+00  7.11585310e+00  6.55767680e+00  8.42119941e+00\n",
      "  4.77653794e+00  4.70899596e+00  8.44305066e+00  2.05232135e+00\n",
      "  5.10966610e-01  2.78608877e+00  5.55062571e+00  4.11519226e+00\n",
      "  7.77461301e+00  6.87195654e-01  1.09490671e+01  1.04462172e+01\n",
      "  3.46799039e+00  4.56050276e+00 -4.98896254e-01  3.83111363e+00\n",
      " -3.33663214e+00  1.21570669e+01  4.17095711e+00  9.76880214e+00\n",
      "  4.50747766e+00  1.00366306e+01  1.06110589e+01  1.20508685e+00\n",
      "  2.60435668e+00  6.73727341e+00  2.25660970e+00  2.56507755e+00\n",
      "  4.20332401e+00 -2.62211704e+00  2.36375024e+00  1.20369778e+01\n",
      "  5.97104447e-01  3.32995436e+00  5.30546142e-02  6.67468545e+00\n",
      "  7.57881345e+00 -4.21939842e+00  2.68312722e+00  3.54473892e+00\n",
      "  5.54178400e+00  7.74963779e+00  1.00263970e+01 -3.47422015e+00\n",
      "  5.67888688e+00  5.92077432e+00  1.00229847e+01  9.01109997e+00\n",
      "  1.04539174e+01  9.42267361e+00  9.47782887e+00  1.45582736e+01\n",
      "  4.51506282e+00  1.85576759e+00  5.35832496e+00  6.33665917e+00\n",
      "  9.57227619e+00  1.31607330e+01  5.32768407e-01  7.15876175e+00\n",
      "  9.64244188e+00  9.17148711e-02  2.23981239e+00 -1.56473725e-01\n",
      "  1.14200645e+01  1.53385501e+01  7.42152192e+00  3.25544952e+00\n",
      "  6.95149516e+00  5.23403057e+00  7.69701225e+00  5.61282984e+00\n",
      "  5.77350485e+00  9.00432888e+00 -3.40302774e-01 -1.65782356e+00\n",
      "  5.55321569e-01  5.47983879e+00 -1.40906189e+00  5.82943777e+00\n",
      "  9.76431438e+00  2.86874887e+00  8.45532488e+00  5.16344972e+00\n",
      "  1.10459647e+01  9.07944728e+00  5.14063269e+00  5.61843182e+00\n",
      "  1.31677769e+01  5.96421052e+00  3.99271270e+00  1.37398105e+00\n",
      "  8.49122089e+00  6.19467624e+00 -3.97536723e+00  5.20631817e+00\n",
      "  4.45478693e+00  6.08862119e+00  9.37320304e+00  1.38122138e+01\n",
      "  4.87484281e+00  6.53445161e+00  9.96034903e+00  4.04191255e+00\n",
      "  5.71755238e+00  6.56775604e+00 -4.75038461e+00 -2.88369819e-01\n",
      "  3.50340030e+00  9.95978775e+00  3.50342205e+00  5.31177923e+00\n",
      " -3.00238221e-01  2.42839346e+00 -1.40118163e+00  3.69169715e+00\n",
      "  1.14172041e+01  8.03324014e+00  6.18598340e+00  1.33257139e+01\n",
      " -4.78492990e+00  7.31249475e+00  1.23451548e+00  6.48782783e+00\n",
      "  9.57891478e+00  5.61252415e+00  3.01069238e+00  1.29037299e+00\n",
      "  1.33192816e+01  6.52312737e+00  2.01934231e+00  4.66684766e+00\n",
      "  8.85557153e+00  3.51955480e+00  6.42061975e+00  1.43787528e+01\n",
      "  4.05217270e+00  8.59836687e+00  1.21802887e+01  6.37230782e+00\n",
      " -1.53587617e+00  3.12047104e+00  1.06487007e+01  5.69648788e+00\n",
      "  1.05994040e+01  5.88307601e+00  4.95546546e+00  4.85945897e-01\n",
      "  8.39019288e+00  8.26699979e+00  6.35991597e+00  2.93807489e+00\n",
      "  5.54631063e+00 -1.69455464e+00  8.33578224e+00 -4.46686895e-01\n",
      "  2.54381873e+00  1.10554915e+01  1.01994774e+01  1.59397640e+00\n",
      "  3.12919673e+00  5.52549244e+00  6.43662940e+00 -7.62819742e-01\n",
      "  8.55121326e-01  5.92091366e+00 -3.97182318e+00  5.28578225e+00\n",
      "  1.69442426e+00  7.47887525e+00  1.98467708e+00 -7.53939925e-01\n",
      "  5.56017616e+00  6.37931543e+00  5.37116706e+00  6.34592112e+00\n",
      "  3.15688523e+00 -2.26378010e+00  1.38939528e+00  6.11072787e+00\n",
      "  3.53157040e+00  4.03513003e+00  5.24315626e+00 -1.54757516e-01\n",
      "  7.15714970e+00  6.95771181e+00  6.36626509e-01  1.07324588e+01\n",
      " -1.65262410e-01  3.71031895e+00  5.03826542e+00  1.43139432e+00\n",
      "  1.18828647e+01  1.76464256e-01 -4.86646965e+00 -1.69074382e+00\n",
      "  8.43987871e-01  9.66159413e+00  9.41747337e+00  7.16130646e+00\n",
      "  3.69528581e+00  1.22501457e+01  1.43925025e+00  4.59088380e+00\n",
      "  1.92819342e+00  8.89225827e+00 -2.71631305e+00  6.68506857e-01\n",
      "  8.29619588e+00 -8.31828334e-01  3.49443294e-01  9.16376459e+00\n",
      "  1.13021688e+01  7.56842795e+00  1.21920716e+01  6.72005110e+00\n",
      "  5.49565455e+00  2.63437206e+00  8.43087443e+00  3.60655220e+00\n",
      "  1.09485198e+01 -2.90199557e+00  6.80211286e+00  5.78969283e+00\n",
      "  8.01708910e+00  3.89262513e+00  5.57617427e+00  1.35235664e+00\n",
      "  2.80848076e+00  1.02415910e+01  4.30995112e+00  2.49085853e-01\n",
      "  2.54041994e+00 -1.25053851e+00  3.10460751e+00  6.43998052e+00\n",
      "  1.65974052e+00 -2.71498808e+00  1.28428682e+01  9.03328556e+00\n",
      "  2.78145314e+00  3.76071670e+00  6.38580883e+00  1.66443024e-01\n",
      "  5.04264512e+00  8.46178436e+00  5.83624163e+00  4.80207136e+00\n",
      "  7.37240860e+00  1.56604265e+00  7.11696085e+00 -8.31120667e+00\n",
      "  1.17751972e+01  7.94740727e+00  5.16272174e+00  6.01233911e+00\n",
      "  1.03101455e+01  5.01251349e+00  1.27281249e+01  5.16671311e-01\n",
      "  4.20309069e+00  8.76153519e-01  2.92158737e+00  1.50564324e+00\n",
      " -6.35184234e-01  5.88027286e+00  1.02393855e+01  3.54586536e+00\n",
      "  1.08159135e+01  1.28068141e+01  4.35133224e+00  1.19772907e+01\n",
      "  5.82416670e-01  5.32630138e+00  5.68998885e+00  7.09267719e+00\n",
      "  4.75388405e-01  4.25119527e+00  1.08489431e+01  9.98329853e+00\n",
      "  9.64737305e+00  4.91640867e+00  2.42894252e-01  7.36749108e+00\n",
      "  6.93681465e+00  5.97647335e+00  3.81226193e+00  1.18133170e+01\n",
      " -1.14743679e-02  1.02711926e+01 -5.96011368e+00  4.02212098e+00\n",
      "  5.14951434e+00  4.64820359e+00  8.54690379e+00  1.54309549e+01\n",
      "  1.11464351e+01  9.27836980e+00  6.41437041e+00  1.27786748e+00\n",
      "  1.14095269e+01  1.42062175e+01  1.41979401e+01  8.22192590e+00\n",
      "  6.65876894e+00  4.45318184e-01  6.83792971e+00  4.56630811e+00\n",
      "  5.74245059e+00 -2.24424280e+00  6.66631721e+00  3.65173343e+00\n",
      "  5.33302836e+00  6.64842989e+00 -2.95744747e+00  6.24224431e+00\n",
      "  8.25520234e+00  9.46977572e+00  8.09995331e+00  3.09075149e+00\n",
      "  5.47280688e+00  2.60652164e+00  8.01446156e+00  6.24238634e+00\n",
      "  3.15362704e+00  2.78414299e+00  1.81109354e+00  2.67037232e+00\n",
      " -4.21426035e-01  6.01903285e+00  3.86805056e+00  5.26277786e+00\n",
      "  8.26536946e+00  1.13155672e+01  9.77889957e+00  4.64953328e+00\n",
      "  3.17829731e-01  6.28924156e+00  1.04397593e+01  3.37860480e+00\n",
      "  6.95082591e+00  1.08047289e+01  7.67136442e+00  1.19384783e+01\n",
      "  1.12995040e+01  1.64017522e+00  2.18619961e+00  8.66688704e-01\n",
      "  8.00091851e+00  8.62688157e+00  8.12959132e+00  3.00177093e+00\n",
      "  7.50816218e+00  8.75579722e+00  6.39341491e+00  1.29740787e+01\n",
      "  6.46665591e+00  5.74039624e+00  5.64011459e+00 -7.78425068e+00\n",
      "  5.98744935e+00  2.74520106e+00  3.30369006e+00  7.59859550e+00\n",
      "  2.49013637e+00  3.03929323e-02  3.74791718e+00  6.51495003e+00\n",
      " -2.77612007e+00 -5.86483615e-01  5.01568680e+00  2.00720425e+00\n",
      "  4.28378544e+00  1.06418378e+01  9.32647739e+00  8.74894114e+00\n",
      "  1.95850188e+00  1.03749892e+00  1.01896814e+01  9.18824851e+00\n",
      " -3.29814421e+00  1.23680921e+01  4.15515016e+00  4.72130404e+00\n",
      "  7.25857383e+00  4.90905519e+00  3.18181699e+00  1.16604829e+01\n",
      "  9.97521249e+00  4.05648409e-01 -1.70614709e+00  5.51773090e+00\n",
      "  3.27184574e+00  8.11803388e+00  1.19560618e+01  3.94386798e+00\n",
      "  6.13677790e+00  2.01015514e+00  9.01903724e+00  9.88458540e+00\n",
      "  2.08766316e+00  6.42527102e+00  4.63163988e+00 -2.23745329e+00\n",
      "  3.93614590e+00  3.76714582e+00  9.84375387e-01 -7.19465868e-01\n",
      "  1.10347972e+01  1.23287874e+01  6.66755417e+00  1.09451708e+01\n",
      "  3.69642571e+00  7.29172592e+00  1.37352884e+01  1.35635371e+01\n",
      " -9.31735060e-01  6.24644629e+00  5.78112838e+00  5.10101437e+00\n",
      "  6.09167890e+00 -1.33402163e+00 -6.08870695e+00  6.71133805e+00\n",
      "  1.16393571e+01 -1.27275829e+00 -2.76417792e-01  3.73944751e+00\n",
      " -3.94125524e-01  8.17103070e+00  1.28977442e+01  8.78174367e+00\n",
      "  1.09542171e+00  6.69995427e+00  5.31055569e+00  2.66587185e+00\n",
      " -7.84483270e-01  2.49275160e+00  4.82915898e+00  7.69465931e+00\n",
      "  4.40927442e+00  4.25388942e+00  1.11846108e+01  3.90111349e+00\n",
      "  4.53659201e+00  1.15568769e+01  3.79163842e+00  1.71640626e+00\n",
      "  1.16430928e+01  1.13624002e+01  5.92522461e+00  5.42139888e+00\n",
      "  7.67289966e+00  7.80218803e+00  4.37649109e+00  8.43269208e+00\n",
      "  6.06138951e+00  1.94383695e+00  2.92067671e+00  1.02599230e+01\n",
      " -1.03426722e+00  3.91456535e+00  1.06764473e+01  2.10423764e+00\n",
      "  5.48705584e+00  1.09270460e+01  5.48093809e+00  1.74802182e+01\n",
      "  1.08658677e+01  9.65769871e+00  6.08230272e+00  3.81728853e+00\n",
      "  1.07156522e+01  5.04423962e+00 -1.77944779e+00  6.00782819e+00\n",
      "  3.26213727e+00  8.41261080e+00 -5.45976992e-01  2.03889416e+00\n",
      " -1.86598624e+00  6.79752706e+00  7.57970501e+00 -3.26109226e-01\n",
      "  7.95208407e+00  5.90735617e+00  1.38192967e+00  8.70097983e+00\n",
      "  4.68270768e+00  9.67023947e+00  7.83598242e+00  4.96569583e+00\n",
      "  2.15491845e+00  8.19519766e+00  7.02987734e+00  8.26003443e+00\n",
      "  2.28274363e+00  2.21087401e+00  3.81737762e+00 -2.78459000e+00\n",
      "  4.47439317e+00  1.41070820e+01  1.14370333e+01  3.87996203e+00\n",
      "  3.08378664e+00 -5.55599278e+00  1.48517478e+00  7.72595622e+00\n",
      "  3.62058269e+00  9.44512290e+00  5.90094630e+00  1.16626903e+01\n",
      "  2.75221915e+00  7.48488140e+00  5.41551172e+00  4.53503420e+00\n",
      " -2.71478050e-01  1.46896971e+00  5.89154839e+00  9.70334918e+00\n",
      "  4.71062165e+00  7.98068815e+00  2.11556720e+00  3.63867230e+00\n",
      " -3.18540006e+00 -3.15987250e+00  2.59187918e+00  1.39651673e+01\n",
      "  8.40671335e+00  1.80751029e+00  7.29070742e+00  2.75997016e+00\n",
      "  8.10243055e+00  9.23207910e-01 -4.32138864e+00  7.83893899e+00\n",
      " -8.66938308e+00  6.06693794e+00  4.61739700e+00  5.64293591e+00\n",
      "  1.53232318e+00 -2.88432570e+00  8.00856615e+00  1.02107032e+01\n",
      "  6.94527427e+00  4.15847742e+00  3.68134518e+00  4.19384875e+00\n",
      "  5.42593559e+00  5.91031291e+00  6.26825792e+00  7.02249009e+00\n",
      "  4.68050903e-01  2.46481810e+00  3.30070347e+00 -7.37073571e-02\n",
      "  1.24376156e+01  2.72778441e+00  7.77292035e+00  9.24329484e-01\n",
      "  8.35072938e+00  2.14386065e+00 -1.94976637e+00  1.41796898e+00\n",
      "  4.25692967e+00  9.86472551e+00  4.54112318e+00  1.12184212e+00\n",
      "  2.72800174e+00  6.33984111e+00  6.12390491e+00  3.20379682e+00\n",
      "  3.91215426e+00  1.14114630e+00  2.73732383e+00  7.08230376e+00\n",
      "  1.80877736e+00  4.99958374e+00  9.32311843e+00  4.11539536e+00\n",
      " -5.00016042e-01  1.71497404e+00  8.34911950e+00  9.52127570e+00\n",
      "  7.60550487e+00  1.38778206e+01  4.71459077e-01  7.86658451e+00\n",
      "  9.70073096e+00  3.28214851e+00  4.16285298e+00  6.24228366e+00\n",
      "  1.35879566e+00  5.80208737e+00  6.81758503e+00  6.56236810e+00\n",
      " -2.07651855e+00  1.56461808e+00  4.15804375e+00  1.23401453e+01\n",
      "  1.02808697e+01  8.59588898e+00  4.07414083e+00  7.15353115e-01\n",
      "  2.66164998e+00  2.25764386e-01  7.42877770e+00  2.36311941e+00\n",
      "  2.57643763e+00  1.50830019e+01  8.73835443e+00  6.97575275e+00\n",
      " -6.92667751e+00  6.85188072e+00  3.22261140e+00  6.16707137e-01\n",
      "  4.82252683e+00 -6.25829502e-01  9.53131667e+00  1.28382648e+00\n",
      "  2.50075242e+00  2.44272944e+00  2.47426230e+00 -2.85183004e+00\n",
      "  7.66600397e+00  8.89231334e+00  7.14087856e+00  1.05971702e+01\n",
      " -3.88105621e+00  4.98066337e+00  1.94265445e+00 -3.09507850e+00\n",
      "  6.52100330e+00  4.38376928e+00 -2.72090403e+00  9.90464698e+00\n",
      "  4.34012027e+00  4.47575349e+00  1.22405764e+01  1.40895493e+01\n",
      "  4.79848442e+00  6.44174978e+00  7.64495600e+00  1.06420512e+00\n",
      "  5.71102693e+00  5.35617520e+00  6.55090183e+00  3.22687798e+00\n",
      "  6.75389970e+00  6.68303233e+00  1.31396027e+01  4.72307851e+00\n",
      " -9.83295378e-01  4.09432013e+00 -1.57946886e+00  4.29713121e+00\n",
      "  8.00081452e+00  2.42984198e+00  4.64281873e-02  9.57778682e+00\n",
      "  2.97826771e+00  9.63381581e+00  5.51854735e+00  4.08434461e+00\n",
      "  5.66227981e+00  1.31303596e+01  8.10225863e+00  1.43170630e+00\n",
      "  1.29658996e+01  7.03005375e+00 -1.52681257e+00  9.29194914e+00\n",
      " -2.43027597e+00  7.34683705e+00  6.84723652e+00  4.26541846e+00\n",
      "  8.30143187e+00  7.45799088e+00  5.90705449e+00  2.90306211e+00\n",
      "  3.87742357e-01  7.40379477e+00  8.52409261e+00  2.37066611e+00\n",
      "  2.11763171e+00  6.10299041e-01  7.13868895e-01  2.14464955e+00\n",
      "  7.49266246e+00  4.12370748e+00  1.19950209e+01  8.07683216e+00\n",
      "  9.95979511e-01  4.39265360e-01  9.66270881e+00  3.30842336e+00\n",
      "  1.19074335e+00 -2.26451675e+00 -7.52964864e-01  4.14557828e+00\n",
      "  4.47507448e+00  2.49783962e+00  2.88662726e-01  4.43791452e+00\n",
      "  3.27589550e+00  8.13565186e-01  4.36563886e+00  4.37175794e+00\n",
      "  8.71741194e+00  9.43104831e+00 -1.01121801e+00  3.14967195e-01\n",
      "  3.11632149e-01  1.94080836e+00  6.64668748e+00  1.35048459e+01\n",
      "  6.60679257e+00  5.13523409e+00  4.85248601e+00 -5.10563218e+00\n",
      "  7.64925321e+00  3.79100248e+00  3.73035265e+00  8.85883101e-02\n",
      " -4.54676117e+00  1.87066720e+00  6.80102725e+00 -3.09060834e-01\n",
      "  8.70037786e+00  2.37214332e+00  1.25192608e+00  2.77982567e+00\n",
      "  6.27164733e+00  3.42827462e+00  1.12142165e+01  9.62019655e+00\n",
      "  2.08422008e-01  1.05257069e+01  9.12288119e-01  8.64585657e+00\n",
      "  7.59543122e+00  3.07637851e+00  1.63297366e+01  8.64942086e+00\n",
      "  4.30247278e+00  2.67097952e+00  1.87188000e+00 -1.44035103e-01\n",
      "  5.98391999e+00  2.94659790e+00  6.30698742e+00  6.67869126e+00\n",
      "  3.49003098e+00 -6.38551241e-01  9.98270653e+00  3.89368774e+00\n",
      "  3.93058856e+00  6.62945679e+00  8.45690442e-01 -5.87115807e-01\n",
      "  6.23700066e+00  5.70280967e+00  4.93546710e+00  1.10933454e+01\n",
      "  8.11032244e+00  6.33239499e+00  9.20189587e+00  1.22606269e+01\n",
      "  1.42780030e+00  1.18661904e+01  1.64390750e+00 -3.02960455e-01\n",
      "  1.19836562e+01  1.54034999e+00  1.96002677e+00  4.50831914e+00\n",
      "  1.13191261e+01  1.64303560e+00  2.28214401e+00  1.68651717e+00\n",
      "  2.84491189e+00  4.44766938e+00  2.28058629e+00  2.19171663e+00\n",
      "  4.97359122e+00  7.74247172e+00  4.67899366e+00  1.32898786e+01\n",
      "  1.14487738e+01 -6.14838497e-02  6.31410730e+00  9.91717825e+00\n",
      "  1.04294341e+01  9.55368789e+00  2.40852034e+00 -4.51079894e+00\n",
      "  5.46278265e+00  5.95085550e+00 -6.60799080e-01  1.83563718e+00\n",
      "  9.12150946e+00  9.55010237e+00  6.72545051e+00  8.37465637e+00\n",
      "  1.04171975e+01  4.08128223e+00  2.56653721e+00  4.20770732e+00\n",
      "  9.32988510e+00  4.96463198e+00  4.51020190e+00  6.34860592e+00\n",
      "  4.10232467e+00  4.27632950e+00  7.52154288e+00  1.98399753e+00\n",
      "  1.18573438e+01  8.99834437e+00  9.07059477e+00  4.80022370e-01\n",
      "  8.57902264e+00 -2.83895956e+00  6.53868301e+00  1.46766688e+00\n",
      "  1.41083454e+00  4.92919416e+00  9.13797138e+00  8.84383753e+00\n",
      "  8.71967735e+00  1.74783917e-01  1.04376554e+01 -4.95422492e+00\n",
      "  8.39080973e+00  4.64435450e+00 -6.70500471e-01  4.08431274e-01\n",
      "  6.14058622e+00  7.97328027e+00  1.02356838e+00  5.98941197e+00\n",
      "  8.12241293e-01  1.23444414e+01  6.71701453e+00  2.36077281e+00\n",
      "  7.03569030e+00  1.47820449e+01 -1.48921065e+00  2.07572826e+00\n",
      "  6.54492962e+00  2.27339157e+01  3.35118543e+00  6.64020569e+00\n",
      " -1.28566609e+00  6.20063015e+00  1.57830775e+00  6.88947986e+00\n",
      "  2.45568160e+00  1.12251649e+01  4.97289275e+00  6.23335222e+00\n",
      "  6.01089410e+00  8.63421468e+00  6.91775281e-01  9.69039060e+00\n",
      "  9.58659454e+00  1.76325710e+00  1.53590506e+01 -1.31581627e+00\n",
      " -8.13554612e-01  4.87722299e+00  9.90649635e+00  4.04940607e+00\n",
      "  8.88091399e+00  1.06364949e+01 -6.36763348e-01  5.94341316e+00\n",
      "  1.13047383e+01  6.38408083e+00  2.97212023e+00  3.64944724e+00\n",
      " -3.42668473e+00  5.63379417e+00  3.21161008e+00  1.11397282e+00]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, true_weights, true_bias)\n",
    "print(\"y_pred:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "   # Your code here\n",
    "    n = len(y_true)\n",
    "    mse = np.sum((y_pred - y_true) ** 2) / n\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.26357415817355806\n"
     ]
    }
   ],
   "source": [
    "loss = compute_mse(y, predict(X, true_weights, true_bias))\n",
    "print(f\"MSE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n = len(y)\n",
    "    gradient_w = (2 / n) * X.T.dot(y_pred - y)\n",
    "    gradient_b = (2 / n) * np.sum(y_pred - y)\n",
    "    return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01697225  0.02900164  0.03877828] 0.018719217786772324\n"
     ]
    }
   ],
   "source": [
    "gradient_w, gradient_b = compute_gradients(X, y, predict(X, true_weights, true_bias))\n",
    "print(gradient_w, gradient_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1.]), array([0.]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.ones((3))\n",
    "b = np.array([0.0])\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), (1,))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, w.shape, b.shape # Should be ((1000, 3), (3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-2.85186442,  9.29304439, -1.0522734 ]), -10.19197986236907)\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, w, b) # Initial predictions\n",
    "print(compute_gradients(X, y, y_pred)) # Should print gradients (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step = 25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        # Compute predictions\n",
    "        y_pred = predict(X, w, b)\n",
    "\n",
    "        # Calculate gradients\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_pred)\n",
    "\n",
    "        # Gradient descent update\n",
    "        w -= learning_rate * grad_w\n",
    "        b -= learning_rate * grad_b\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            # Calculate loss (MSE)\n",
    "            loss = compute_mse(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.871205\n",
      "Iteration   25 | Loss: 16.066821\n",
      "Iteration   50 | Loss: 5.745055\n",
      "Iteration   75 | Loss: 2.166888\n",
      "Iteration  100 | Loss: 0.925067\n",
      "Iteration  125 | Loss: 0.493531\n",
      "Iteration  150 | Loss: 0.343353\n",
      "Iteration  175 | Loss: 0.291002\n",
      "Iteration  200 | Loss: 0.272720\n",
      "Iteration  225 | Loss: 0.266321\n",
      "Iteration  250 | Loss: 0.264077\n",
      "Iteration  275 | Loss: 0.263287\n",
      "Iteration  300 | Loss: 0.263008\n",
      "Iteration  325 | Loss: 0.262910\n",
      "Iteration  350 | Loss: 0.262875\n",
      "Iteration  375 | Loss: 0.262862\n",
      "Iteration  400 | Loss: 0.262858\n",
      "Iteration  425 | Loss: 0.262856\n",
      "Iteration  450 | Loss: 0.262856\n",
      "Iteration  475 | Loss: 0.262856\n",
      "Iteration  499 | Loss: 0.262855\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y, \n",
    "    learning_rate=0.01, \n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "True weights:    [ 2.  -3.5  1.5]\n",
      "Learned weights: [ 2.00856936 -3.51317294  1.4809054 ]\n",
      "\n",
      "True bias:    5.0\n",
      "Learned bias: 4.9908\n"
     ]
    }
   ],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges:\n",
      "  Feature 1: [-3241.27, 3852.73]\n",
      "  Feature 2: [-0.00, 0.00]\n",
      "  Feature 3: [-2.90, 2.60]\n"
     ]
    }
   ],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + 5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 85.696115\n",
      "Iteration   25 | Loss: 871155159202081111520571956442045941119879752400970050021201808697372616902517776369758662489941122009264930032502356888346629332618898026256512926205772115572748248142770935687381324059140750992520089259333243633664.000000\n",
      "Iteration   50 | Loss: inf\n",
      "Iteration   75 | Loss: nan\n",
      "Iteration   99 | Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18452\\2049492081.py:14: RuntimeWarning: overflow encountered in square\n",
      "  mse = np.sum((y_pred - y_true) ** 2) / n\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18452\\2501940967.py:40: RuntimeWarning: invalid value encountered in subtract\n",
      "  w -= learning_rate * grad_w\n"
     ]
    }
   ],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_standardized = (X - mean) / std\n",
    "    return X_standardized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature ranges:\n",
      "  Feature 1: [-3.31, 3.92]\n",
      "  Feature 2: [-2.79, 2.66]\n",
      "  Feature 3: [-2.98, 2.47]\n",
      "Iteration    0 | Loss: 29.787905\n",
      "Iteration   25 | Loss: 0.240939\n",
      "Iteration   50 | Loss: 0.240479\n",
      "Iteration   75 | Loss: 0.240479\n",
      "Iteration  100 | Loss: 0.240479\n",
      "Iteration  125 | Loss: 0.240479\n",
      "Iteration  150 | Loss: 0.240479\n",
      "Iteration  175 | Loss: 0.240479\n",
      "Iteration  200 | Loss: 0.240479\n",
      "Iteration  225 | Loss: 0.240479\n",
      "Iteration  250 | Loss: 0.240479\n",
      "Iteration  275 | Loss: 0.240479\n",
      "Iteration  300 | Loss: 0.240479\n",
      "Iteration  325 | Loss: 0.240479\n",
      "Iteration  350 | Loss: 0.240479\n",
      "Iteration  375 | Loss: 0.240479\n",
      "Iteration  400 | Loss: 0.240479\n",
      "Iteration  425 | Loss: 0.240479\n",
      "Iteration  450 | Loss: 0.240479\n",
      "Iteration  475 | Loss: 0.240479\n",
      "Iteration  499 | Loss: 0.240479\n"
     ]
    }
   ],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.1, n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        for i in range(n_iterations):\n",
    "            # Your code here\n",
    "            batch_index = np.random.choice(n_samples, size= batch_size, replace= False)\n",
    "\n",
    "            X_batch = X[batch_index]\n",
    "            y_batch = y[batch_index]\n",
    "\n",
    "            y_pred = predict(X_batch, w, b)\n",
    "            grad_w, grad_b = compute_gradients(X_batch, y_batch, y_pred)\n",
    "            \n",
    "            w = w - (learning_rate * grad_w)\n",
    "            b = b - (learning_rate * grad_b)\n",
    "            if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "                y_true_pred = predict(X, w, b)\n",
    "                loss = compute_mse(y, y_true_pred)\n",
    "                loss_history.append(loss)\n",
    "                print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "            \n",
    "            if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "                print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 43.812582\n",
      "Iteration    0 | Loss: 43.812582\n",
      "Iteration   50 | Loss: 5.575376\n",
      "Iteration   50 | Loss: 5.575376\n",
      "Iteration  100 | Loss: 0.904171\n",
      "Iteration  100 | Loss: 0.904171\n",
      "Iteration  150 | Loss: 0.339837\n",
      "Iteration  150 | Loss: 0.339837\n",
      "Iteration  199 | Loss: 0.274464\n",
      "Iteration  199 | Loss: 0.274464\n",
      "Iteration    0 | Loss: 0.274014\n",
      "Iteration    0 | Loss: 0.274014\n",
      "Iteration   50 | Loss: 0.264058\n",
      "Iteration   50 | Loss: 0.264058\n",
      "Iteration  100 | Loss: 0.263019\n",
      "Iteration  100 | Loss: 0.263019\n",
      "Iteration  150 | Loss: 0.263071\n",
      "Iteration  150 | Loss: 0.263071\n",
      "Iteration  199 | Loss: 0.262923\n",
      "Iteration  199 | Loss: 0.262923\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration   50 | Loss: 0.263272\n",
      "Iteration   50 | Loss: 0.263272\n",
      "Iteration  100 | Loss: 0.263070\n",
      "Iteration  100 | Loss: 0.263070\n",
      "Iteration  150 | Loss: 0.263063\n",
      "Iteration  150 | Loss: 0.263063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  199 | Loss: 0.262888\n",
      "Iteration  199 | Loss: 0.262888\n",
      "Iteration    0 | Loss: 0.262880\n",
      "Iteration    0 | Loss: 0.262880\n",
      "Iteration   50 | Loss: 0.262933\n",
      "Iteration   50 | Loss: 0.262933\n",
      "Iteration  100 | Loss: 0.263047\n",
      "Iteration  100 | Loss: 0.263047\n",
      "Iteration  150 | Loss: 0.262930\n",
      "Iteration  150 | Loss: 0.262930\n",
      "Iteration  199 | Loss: 0.262992\n",
      "Iteration  199 | Loss: 0.262992\n",
      "Iteration    0 | Loss: 0.263004\n",
      "Iteration    0 | Loss: 0.263004\n",
      "Iteration   50 | Loss: 0.262948\n",
      "Iteration   50 | Loss: 0.262948\n",
      "Iteration  100 | Loss: 0.262873\n",
      "Iteration  100 | Loss: 0.262873\n",
      "Iteration  150 | Loss: 0.263010\n",
      "Iteration  150 | Loss: 0.263010\n",
      "Iteration  199 | Loss: 0.262976\n",
      "Iteration  199 | Loss: 0.262976\n",
      "Iteration    0 | Loss: 0.263004\n",
      "Iteration    0 | Loss: 0.263004\n",
      "Iteration   50 | Loss: 0.262996\n",
      "Iteration   50 | Loss: 0.262996\n",
      "Iteration  100 | Loss: 0.262901\n",
      "Iteration  100 | Loss: 0.262901\n",
      "Iteration  150 | Loss: 0.262889\n",
      "Iteration  150 | Loss: 0.262889\n",
      "Iteration  199 | Loss: 0.262888\n",
      "Iteration  199 | Loss: 0.262888\n",
      "Iteration    0 | Loss: 0.262917\n",
      "Iteration    0 | Loss: 0.262917\n",
      "Iteration   50 | Loss: 0.262915\n",
      "Iteration   50 | Loss: 0.262915\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  150 | Loss: 0.262902\n",
      "Iteration  150 | Loss: 0.262902\n",
      "Iteration  199 | Loss: 0.262928\n",
      "Iteration  199 | Loss: 0.262928\n",
      "Iteration    0 | Loss: 0.262929\n",
      "Iteration    0 | Loss: 0.262929\n",
      "Iteration   50 | Loss: 0.263032\n",
      "Iteration   50 | Loss: 0.263032\n",
      "Iteration  100 | Loss: 0.263066\n",
      "Iteration  100 | Loss: 0.263066\n",
      "Iteration  150 | Loss: 0.263006\n",
      "Iteration  150 | Loss: 0.263006\n",
      "Iteration  199 | Loss: 0.263191\n",
      "Iteration  199 | Loss: 0.263191\n",
      "Iteration    0 | Loss: 0.263218\n",
      "Iteration    0 | Loss: 0.263218\n",
      "Iteration   50 | Loss: 0.262984\n",
      "Iteration   50 | Loss: 0.262984\n",
      "Iteration  100 | Loss: 0.262947\n",
      "Iteration  100 | Loss: 0.262947\n",
      "Iteration  150 | Loss: 0.262955\n",
      "Iteration  150 | Loss: 0.262955\n",
      "Iteration  199 | Loss: 0.262901\n",
      "Iteration  199 | Loss: 0.262901\n",
      "Iteration    0 | Loss: 0.262896\n",
      "Iteration    0 | Loss: 0.262896\n",
      "Iteration   50 | Loss: 0.262872\n",
      "Iteration   50 | Loss: 0.262872\n",
      "Iteration  100 | Loss: 0.262915\n",
      "Iteration  100 | Loss: 0.262915\n",
      "Iteration  150 | Loss: 0.262878\n",
      "Iteration  150 | Loss: 0.262878\n",
      "Iteration  199 | Loss: 0.262947\n",
      "Iteration  199 | Loss: 0.262947\n",
      "Iteration    0 | Loss: 0.262942\n",
      "Iteration    0 | Loss: 0.262942\n",
      "Iteration   50 | Loss: 0.263114\n",
      "Iteration   50 | Loss: 0.263114\n",
      "Iteration  100 | Loss: 0.262991\n",
      "Iteration  100 | Loss: 0.262991\n",
      "Iteration  150 | Loss: 0.263466\n",
      "Iteration  150 | Loss: 0.263466\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration    0 | Loss: 0.262990\n",
      "Iteration    0 | Loss: 0.262990\n",
      "Iteration   50 | Loss: 0.262879\n",
      "Iteration   50 | Loss: 0.262879\n",
      "Iteration  100 | Loss: 0.262909\n",
      "Iteration  100 | Loss: 0.262909\n",
      "Iteration  150 | Loss: 0.262891\n",
      "Iteration  150 | Loss: 0.262891\n",
      "Iteration  199 | Loss: 0.262929\n",
      "Iteration  199 | Loss: 0.262929\n",
      "Iteration    0 | Loss: 0.262933\n",
      "Iteration    0 | Loss: 0.262933\n",
      "Iteration   50 | Loss: 0.263022\n",
      "Iteration   50 | Loss: 0.263022\n",
      "Iteration  100 | Loss: 0.263074\n",
      "Iteration  100 | Loss: 0.263074\n",
      "Iteration  150 | Loss: 0.262971\n",
      "Iteration  150 | Loss: 0.262971\n",
      "Iteration  199 | Loss: 0.262890\n",
      "Iteration  199 | Loss: 0.262890\n",
      "Iteration    0 | Loss: 0.262887\n",
      "Iteration    0 | Loss: 0.262887\n",
      "Iteration   50 | Loss: 0.262973\n",
      "Iteration   50 | Loss: 0.262973\n",
      "Iteration  100 | Loss: 0.263140\n",
      "Iteration  100 | Loss: 0.263140\n",
      "Iteration  150 | Loss: 0.262978\n",
      "Iteration  150 | Loss: 0.262978\n",
      "Iteration  199 | Loss: 0.263037\n",
      "Iteration  199 | Loss: 0.263037\n",
      "Iteration    0 | Loss: 0.263010\n",
      "Iteration    0 | Loss: 0.263010\n",
      "Iteration   50 | Loss: 0.262974\n",
      "Iteration   50 | Loss: 0.262974\n",
      "Iteration  100 | Loss: 0.263044\n",
      "Iteration  100 | Loss: 0.263044\n",
      "Iteration  150 | Loss: 0.262995\n",
      "Iteration  150 | Loss: 0.262995\n",
      "Iteration  199 | Loss: 0.262889\n",
      "Iteration  199 | Loss: 0.262889\n",
      "Iteration    0 | Loss: 0.262895\n",
      "Iteration    0 | Loss: 0.262895\n",
      "Iteration   50 | Loss: 0.262985\n",
      "Iteration   50 | Loss: 0.262985\n",
      "Iteration  100 | Loss: 0.262948\n",
      "Iteration  100 | Loss: 0.262948\n",
      "Iteration  150 | Loss: 0.262975\n",
      "Iteration  150 | Loss: 0.262975\n",
      "Iteration  199 | Loss: 0.263153\n",
      "Iteration  199 | Loss: 0.263153\n",
      "Iteration    0 | Loss: 0.263157\n",
      "Iteration    0 | Loss: 0.263157\n",
      "Iteration   50 | Loss: 0.262868\n",
      "Iteration   50 | Loss: 0.262868\n",
      "Iteration  100 | Loss: 0.262964\n",
      "Iteration  100 | Loss: 0.262964\n",
      "Iteration  150 | Loss: 0.263090\n",
      "Iteration  150 | Loss: 0.263090\n",
      "Iteration  199 | Loss: 0.262949\n",
      "Iteration  199 | Loss: 0.262949\n",
      "Iteration    0 | Loss: 0.262958\n",
      "Iteration    0 | Loss: 0.262958\n",
      "Iteration   50 | Loss: 0.262908\n",
      "Iteration   50 | Loss: 0.262908\n",
      "Iteration  100 | Loss: 0.262955\n",
      "Iteration  100 | Loss: 0.262955\n",
      "Iteration  150 | Loss: 0.262975\n",
      "Iteration  150 | Loss: 0.262975\n",
      "Iteration  199 | Loss: 0.262925\n",
      "Iteration  199 | Loss: 0.262925\n",
      "Iteration    0 | Loss: 0.262928\n",
      "Iteration    0 | Loss: 0.262928\n",
      "Iteration   50 | Loss: 0.262929\n",
      "Iteration   50 | Loss: 0.262929\n",
      "Iteration  100 | Loss: 0.262912\n",
      "Iteration  100 | Loss: 0.262912\n",
      "Iteration  150 | Loss: 0.262963\n",
      "Iteration  150 | Loss: 0.262963\n",
      "Iteration  199 | Loss: 0.263023\n",
      "Iteration  199 | Loss: 0.263023\n",
      "Iteration    0 | Loss: 0.263006\n",
      "Iteration    0 | Loss: 0.263006\n",
      "Iteration   50 | Loss: 0.263140\n",
      "Iteration   50 | Loss: 0.263140\n",
      "Iteration  100 | Loss: 0.262979\n",
      "Iteration  100 | Loss: 0.262979\n",
      "Iteration  150 | Loss: 0.262968\n",
      "Iteration  150 | Loss: 0.262968\n",
      "Iteration  199 | Loss: 0.262972\n",
      "Iteration  199 | Loss: 0.262972\n",
      "Iteration    0 | Loss: 0.263016\n",
      "Iteration    0 | Loss: 0.263016\n",
      "Iteration   50 | Loss: 0.263152\n",
      "Iteration   50 | Loss: 0.263152\n",
      "Iteration  100 | Loss: 0.262938\n",
      "Iteration  100 | Loss: 0.262938\n",
      "Iteration  150 | Loss: 0.262951\n",
      "Iteration  150 | Loss: 0.262951\n",
      "Iteration  199 | Loss: 0.262883\n",
      "Iteration  199 | Loss: 0.262883\n",
      "Iteration    0 | Loss: 0.262879\n",
      "Iteration    0 | Loss: 0.262879\n",
      "Iteration   50 | Loss: 0.262943\n",
      "Iteration   50 | Loss: 0.262943\n",
      "Iteration  100 | Loss: 0.262942\n",
      "Iteration  100 | Loss: 0.262942\n",
      "Iteration  150 | Loss: 0.263341\n",
      "Iteration  150 | Loss: 0.263341\n",
      "Iteration  199 | Loss: 0.262964\n",
      "Iteration  199 | Loss: 0.262964\n",
      "Iteration    0 | Loss: 0.262999\n",
      "Iteration    0 | Loss: 0.262999\n",
      "Iteration   50 | Loss: 0.262952\n",
      "Iteration   50 | Loss: 0.262952\n",
      "Iteration  100 | Loss: 0.262951\n",
      "Iteration  100 | Loss: 0.262951\n",
      "Iteration  150 | Loss: 0.263076\n",
      "Iteration  150 | Loss: 0.263076\n",
      "Iteration  199 | Loss: 0.262930\n",
      "Iteration  199 | Loss: 0.262930\n",
      "Iteration    0 | Loss: 0.262897\n",
      "Iteration    0 | Loss: 0.262897\n",
      "Iteration   50 | Loss: 0.263029\n",
      "Iteration   50 | Loss: 0.263029\n",
      "Iteration  100 | Loss: 0.263198\n",
      "Iteration  100 | Loss: 0.263198\n",
      "Iteration  150 | Loss: 0.262961\n",
      "Iteration  150 | Loss: 0.262961\n",
      "Iteration  199 | Loss: 0.262932\n",
      "Iteration  199 | Loss: 0.262932\n",
      "Iteration    0 | Loss: 0.262946\n",
      "Iteration    0 | Loss: 0.262946\n",
      "Iteration   50 | Loss: 0.262937\n",
      "Iteration   50 | Loss: 0.262937\n",
      "Iteration  100 | Loss: 0.262941\n",
      "Iteration  100 | Loss: 0.262941\n",
      "Iteration  150 | Loss: 0.262874\n",
      "Iteration  150 | Loss: 0.262874\n",
      "Iteration  199 | Loss: 0.262944\n",
      "Iteration  199 | Loss: 0.262944\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration   50 | Loss: 0.262940\n",
      "Iteration   50 | Loss: 0.262940\n",
      "Iteration  100 | Loss: 0.262893\n",
      "Iteration  100 | Loss: 0.262893\n",
      "Iteration  150 | Loss: 0.262941\n",
      "Iteration  150 | Loss: 0.262941\n",
      "Iteration  199 | Loss: 0.262927\n",
      "Iteration  199 | Loss: 0.262927\n",
      "Iteration    0 | Loss: 0.262926\n",
      "Iteration    0 | Loss: 0.262926\n",
      "Iteration   50 | Loss: 0.262933\n",
      "Iteration   50 | Loss: 0.262933\n",
      "Iteration  100 | Loss: 0.263329\n",
      "Iteration  100 | Loss: 0.263329\n",
      "Iteration  150 | Loss: 0.262921\n",
      "Iteration  150 | Loss: 0.262921\n",
      "Iteration  199 | Loss: 0.262961\n",
      "Iteration  199 | Loss: 0.262961\n",
      "Iteration    0 | Loss: 0.262959\n",
      "Iteration    0 | Loss: 0.262959\n",
      "Iteration   50 | Loss: 0.263002\n",
      "Iteration   50 | Loss: 0.263002\n",
      "Iteration  100 | Loss: 0.263075\n",
      "Iteration  100 | Loss: 0.263075\n",
      "Iteration  150 | Loss: 0.263024\n",
      "Iteration  150 | Loss: 0.263024\n",
      "Iteration  199 | Loss: 0.263003\n",
      "Iteration  199 | Loss: 0.263003\n",
      "Iteration    0 | Loss: 0.263032\n",
      "Iteration    0 | Loss: 0.263032\n",
      "Iteration   50 | Loss: 0.263282\n",
      "Iteration   50 | Loss: 0.263282\n",
      "Iteration  100 | Loss: 0.262993\n",
      "Iteration  100 | Loss: 0.262993\n",
      "Iteration  150 | Loss: 0.263133\n",
      "Iteration  150 | Loss: 0.263133\n",
      "Iteration  199 | Loss: 0.263035\n",
      "Iteration  199 | Loss: 0.263035\n",
      "Iteration    0 | Loss: 0.263089\n",
      "Iteration    0 | Loss: 0.263089\n",
      "Iteration   50 | Loss: 0.262875\n",
      "Iteration   50 | Loss: 0.262875\n",
      "Iteration  100 | Loss: 0.262960\n",
      "Iteration  100 | Loss: 0.262960\n",
      "Iteration  150 | Loss: 0.262988\n",
      "Iteration  150 | Loss: 0.262988\n",
      "Iteration  199 | Loss: 0.262954\n",
      "Iteration  199 | Loss: 0.262954\n",
      "Iteration    0 | Loss: 0.262992\n",
      "Iteration    0 | Loss: 0.262992\n",
      "Iteration   50 | Loss: 0.262973\n",
      "Iteration   50 | Loss: 0.262973\n",
      "Iteration  100 | Loss: 0.263022\n",
      "Iteration  100 | Loss: 0.263022\n",
      "Iteration  150 | Loss: 0.262901\n",
      "Iteration  150 | Loss: 0.262901\n",
      "Iteration  199 | Loss: 0.263048\n",
      "Iteration  199 | Loss: 0.263048\n",
      "Iteration    0 | Loss: 0.263070\n",
      "Iteration    0 | Loss: 0.263070\n",
      "Iteration   50 | Loss: 0.263077\n",
      "Iteration   50 | Loss: 0.263077\n",
      "Iteration  100 | Loss: 0.263004\n",
      "Iteration  100 | Loss: 0.263004\n",
      "Iteration  150 | Loss: 0.263003\n",
      "Iteration  150 | Loss: 0.263003\n",
      "Iteration  199 | Loss: 0.262929\n",
      "Iteration  199 | Loss: 0.262929\n",
      "Iteration    0 | Loss: 0.262936\n",
      "Iteration    0 | Loss: 0.262936\n",
      "Iteration   50 | Loss: 0.263051\n",
      "Iteration   50 | Loss: 0.263051\n",
      "Iteration  100 | Loss: 0.263258\n",
      "Iteration  100 | Loss: 0.263258\n",
      "Iteration  150 | Loss: 0.262887\n",
      "Iteration  150 | Loss: 0.262887\n",
      "Iteration  199 | Loss: 0.262894\n",
      "Iteration  199 | Loss: 0.262894\n",
      "Iteration    0 | Loss: 0.262886\n",
      "Iteration    0 | Loss: 0.262886\n",
      "Iteration   50 | Loss: 0.263124\n",
      "Iteration   50 | Loss: 0.263124\n",
      "Iteration  100 | Loss: 0.263259\n",
      "Iteration  100 | Loss: 0.263259\n",
      "Iteration  150 | Loss: 0.262934\n",
      "Iteration  150 | Loss: 0.262934\n",
      "Iteration  199 | Loss: 0.262946\n",
      "Iteration  199 | Loss: 0.262946\n",
      "Iteration    0 | Loss: 0.262972\n",
      "Iteration    0 | Loss: 0.262972\n",
      "Iteration   50 | Loss: 0.262946\n",
      "Iteration   50 | Loss: 0.262946\n",
      "Iteration  100 | Loss: 0.263095\n",
      "Iteration  100 | Loss: 0.263095\n",
      "Iteration  150 | Loss: 0.262968\n",
      "Iteration  150 | Loss: 0.262968\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration    0 | Loss: 0.262935\n",
      "Iteration    0 | Loss: 0.262935\n",
      "Iteration   50 | Loss: 0.262985\n",
      "Iteration   50 | Loss: 0.262985\n",
      "Iteration  100 | Loss: 0.263078\n",
      "Iteration  100 | Loss: 0.263078\n",
      "Iteration  150 | Loss: 0.263150\n",
      "Iteration  150 | Loss: 0.263150\n",
      "Iteration  199 | Loss: 0.263239\n",
      "Iteration  199 | Loss: 0.263239\n",
      "Iteration    0 | Loss: 0.263239\n",
      "Iteration    0 | Loss: 0.263239\n",
      "Iteration   50 | Loss: 0.262962\n",
      "Iteration   50 | Loss: 0.262962\n",
      "Iteration  100 | Loss: 0.262956\n",
      "Iteration  100 | Loss: 0.262956\n",
      "Iteration  150 | Loss: 0.262970\n",
      "Iteration  150 | Loss: 0.262970\n",
      "Iteration  199 | Loss: 0.262936\n",
      "Iteration  199 | Loss: 0.262936\n",
      "Iteration    0 | Loss: 0.262962\n",
      "Iteration    0 | Loss: 0.262962\n",
      "Iteration   50 | Loss: 0.262938\n",
      "Iteration   50 | Loss: 0.262938\n",
      "Iteration  100 | Loss: 0.262908\n",
      "Iteration  100 | Loss: 0.262908\n",
      "Iteration  150 | Loss: 0.262899\n",
      "Iteration  150 | Loss: 0.262899\n",
      "Iteration  199 | Loss: 0.262903\n",
      "Iteration  199 | Loss: 0.262903\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration   50 | Loss: 0.263088\n",
      "Iteration   50 | Loss: 0.263088\n",
      "Iteration  100 | Loss: 0.263020\n",
      "Iteration  100 | Loss: 0.263020\n",
      "Iteration  150 | Loss: 0.262942\n",
      "Iteration  150 | Loss: 0.262942\n",
      "Iteration  199 | Loss: 0.262949\n",
      "Iteration  199 | Loss: 0.262949\n",
      "Iteration    0 | Loss: 0.262964\n",
      "Iteration    0 | Loss: 0.262964\n",
      "Iteration   50 | Loss: 0.263090\n",
      "Iteration   50 | Loss: 0.263090\n",
      "Iteration  100 | Loss: 0.263031\n",
      "Iteration  100 | Loss: 0.263031\n",
      "Iteration  150 | Loss: 0.263120\n",
      "Iteration  150 | Loss: 0.263120\n",
      "Iteration  199 | Loss: 0.263054\n",
      "Iteration  199 | Loss: 0.263054\n",
      "Iteration    0 | Loss: 0.263069\n",
      "Iteration    0 | Loss: 0.263069\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  150 | Loss: 0.262869\n",
      "Iteration  150 | Loss: 0.262869\n",
      "Iteration  199 | Loss: 0.262886\n",
      "Iteration  199 | Loss: 0.262886\n",
      "Iteration    0 | Loss: 0.262864\n",
      "Iteration    0 | Loss: 0.262864\n",
      "Iteration   50 | Loss: 0.262977\n",
      "Iteration   50 | Loss: 0.262977\n",
      "Iteration  100 | Loss: 0.263003\n",
      "Iteration  100 | Loss: 0.263003\n",
      "Iteration  150 | Loss: 0.262954\n",
      "Iteration  150 | Loss: 0.262954\n",
      "Iteration  199 | Loss: 0.262948\n",
      "Iteration  199 | Loss: 0.262948\n",
      "Iteration    0 | Loss: 0.262946\n",
      "Iteration    0 | Loss: 0.262946\n",
      "Iteration   50 | Loss: 0.262972\n",
      "Iteration   50 | Loss: 0.262972\n",
      "Iteration  100 | Loss: 0.262879\n",
      "Iteration  100 | Loss: 0.262879\n",
      "Iteration  150 | Loss: 0.262907\n",
      "Iteration  150 | Loss: 0.262907\n",
      "Iteration  199 | Loss: 0.262908\n",
      "Iteration  199 | Loss: 0.262908\n",
      "Iteration    0 | Loss: 0.262929\n",
      "Iteration    0 | Loss: 0.262929\n",
      "Iteration   50 | Loss: 0.263071\n",
      "Iteration   50 | Loss: 0.263071\n",
      "Iteration  100 | Loss: 0.262992\n",
      "Iteration  100 | Loss: 0.262992\n",
      "Iteration  150 | Loss: 0.263008\n",
      "Iteration  150 | Loss: 0.263008\n",
      "Iteration  199 | Loss: 0.262890\n",
      "Iteration  199 | Loss: 0.262890\n",
      "Iteration    0 | Loss: 0.262897\n",
      "Iteration    0 | Loss: 0.262897\n",
      "Iteration   50 | Loss: 0.263000\n",
      "Iteration   50 | Loss: 0.263000\n",
      "Iteration  100 | Loss: 0.263042\n",
      "Iteration  100 | Loss: 0.263042\n",
      "Iteration  150 | Loss: 0.263034\n",
      "Iteration  150 | Loss: 0.263034\n",
      "Iteration  199 | Loss: 0.263020\n",
      "Iteration  199 | Loss: 0.263020\n",
      "Iteration    0 | Loss: 0.263038\n",
      "Iteration    0 | Loss: 0.263038\n",
      "Iteration   50 | Loss: 0.263006\n",
      "Iteration   50 | Loss: 0.263006\n",
      "Iteration  100 | Loss: 0.262888\n",
      "Iteration  100 | Loss: 0.262888\n",
      "Iteration  150 | Loss: 0.262974\n",
      "Iteration  150 | Loss: 0.262974\n",
      "Iteration  199 | Loss: 0.263172\n",
      "Iteration  199 | Loss: 0.263172\n",
      "Iteration    0 | Loss: 0.263167\n",
      "Iteration    0 | Loss: 0.263167\n",
      "Iteration   50 | Loss: 0.262858\n",
      "Iteration   50 | Loss: 0.262858\n",
      "Iteration  100 | Loss: 0.263244\n",
      "Iteration  100 | Loss: 0.263244\n",
      "Iteration  150 | Loss: 0.263000\n",
      "Iteration  150 | Loss: 0.263000\n",
      "Iteration  199 | Loss: 0.262951\n",
      "Iteration  199 | Loss: 0.262951\n",
      "Iteration    0 | Loss: 0.262982\n",
      "Iteration    0 | Loss: 0.262982\n",
      "Iteration   50 | Loss: 0.263081\n",
      "Iteration   50 | Loss: 0.263081\n",
      "Iteration  100 | Loss: 0.263031\n",
      "Iteration  100 | Loss: 0.263031\n",
      "Iteration  150 | Loss: 0.262893\n",
      "Iteration  150 | Loss: 0.262893\n",
      "Iteration  199 | Loss: 0.262905\n",
      "Iteration  199 | Loss: 0.262905\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration    0 | Loss: 0.262913\n",
      "Iteration   50 | Loss: 0.262979\n",
      "Iteration   50 | Loss: 0.262979\n",
      "Iteration  100 | Loss: 0.262964\n",
      "Iteration  100 | Loss: 0.262964\n",
      "Iteration  150 | Loss: 0.262973\n",
      "Iteration  150 | Loss: 0.262973\n",
      "Iteration  199 | Loss: 0.262938\n",
      "Iteration  199 | Loss: 0.262938\n",
      "Iteration    0 | Loss: 0.262944\n",
      "Iteration    0 | Loss: 0.262944\n",
      "Iteration   50 | Loss: 0.262906\n",
      "Iteration   50 | Loss: 0.262906\n",
      "Iteration  100 | Loss: 0.263020\n",
      "Iteration  100 | Loss: 0.263020\n",
      "Iteration  150 | Loss: 0.262961\n",
      "Iteration  150 | Loss: 0.262961\n",
      "Iteration  199 | Loss: 0.262974\n",
      "Iteration  199 | Loss: 0.262974\n",
      "Iteration    0 | Loss: 0.262982\n",
      "Iteration    0 | Loss: 0.262982\n",
      "Iteration   50 | Loss: 0.262939\n",
      "Iteration   50 | Loss: 0.262939\n",
      "Iteration  100 | Loss: 0.263076\n",
      "Iteration  100 | Loss: 0.263076\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  199 | Loss: 0.262988\n",
      "Iteration  199 | Loss: 0.262988\n",
      "Iteration    0 | Loss: 0.263057\n",
      "Iteration    0 | Loss: 0.263057\n",
      "Iteration   50 | Loss: 0.262931\n",
      "Iteration   50 | Loss: 0.262931\n",
      "Iteration  100 | Loss: 0.262877\n",
      "Iteration  100 | Loss: 0.262877\n",
      "Iteration  150 | Loss: 0.262910\n",
      "Iteration  150 | Loss: 0.262910\n",
      "Iteration  199 | Loss: 0.262977\n",
      "Iteration  199 | Loss: 0.262977\n",
      "Iteration    0 | Loss: 0.262977\n",
      "Iteration    0 | Loss: 0.262977\n",
      "Iteration   50 | Loss: 0.262988\n",
      "Iteration   50 | Loss: 0.262988\n",
      "Iteration  100 | Loss: 0.263089\n",
      "Iteration  100 | Loss: 0.263089\n",
      "Iteration  150 | Loss: 0.263079\n",
      "Iteration  150 | Loss: 0.263079\n",
      "Iteration  199 | Loss: 0.262894\n",
      "Iteration  199 | Loss: 0.262894\n",
      "Iteration    0 | Loss: 0.262886\n",
      "Iteration    0 | Loss: 0.262886\n",
      "Iteration   50 | Loss: 0.262976\n",
      "Iteration   50 | Loss: 0.262976\n",
      "Iteration  100 | Loss: 0.263128\n",
      "Iteration  100 | Loss: 0.263128\n",
      "Iteration  150 | Loss: 0.263098\n",
      "Iteration  150 | Loss: 0.263098\n",
      "Iteration  199 | Loss: 0.262952\n",
      "Iteration  199 | Loss: 0.262952\n",
      "Iteration    0 | Loss: 0.262953\n",
      "Iteration    0 | Loss: 0.262953\n",
      "Iteration   50 | Loss: 0.262897\n",
      "Iteration   50 | Loss: 0.262897\n",
      "Iteration  100 | Loss: 0.262981\n",
      "Iteration  100 | Loss: 0.262981\n",
      "Iteration  150 | Loss: 0.263006\n",
      "Iteration  150 | Loss: 0.263006\n",
      "Iteration  199 | Loss: 0.262898\n",
      "Iteration  199 | Loss: 0.262898\n",
      "Iteration    0 | Loss: 0.262889\n",
      "Iteration    0 | Loss: 0.262889\n",
      "Iteration   50 | Loss: 0.262915\n",
      "Iteration   50 | Loss: 0.262915\n",
      "Iteration  100 | Loss: 0.263050\n",
      "Iteration  100 | Loss: 0.263050\n",
      "Iteration  150 | Loss: 0.262970\n",
      "Iteration  150 | Loss: 0.262970\n",
      "Iteration  199 | Loss: 0.263007\n",
      "Iteration  199 | Loss: 0.263007\n",
      "Iteration    0 | Loss: 0.262988\n",
      "Iteration    0 | Loss: 0.262988\n",
      "Iteration   50 | Loss: 0.262917\n",
      "Iteration   50 | Loss: 0.262917\n",
      "Iteration  100 | Loss: 0.262942\n",
      "Iteration  100 | Loss: 0.262942\n",
      "Iteration  150 | Loss: 0.262937\n",
      "Iteration  150 | Loss: 0.262937\n",
      "Iteration  199 | Loss: 0.262998\n",
      "Iteration  199 | Loss: 0.262998\n",
      "Iteration    0 | Loss: 0.263032\n",
      "Iteration    0 | Loss: 0.263032\n",
      "Iteration   50 | Loss: 0.263106\n",
      "Iteration   50 | Loss: 0.263106\n",
      "Iteration  100 | Loss: 0.262995\n",
      "Iteration  100 | Loss: 0.262995\n",
      "Iteration  150 | Loss: 0.263037\n",
      "Iteration  150 | Loss: 0.263037\n",
      "Iteration  199 | Loss: 0.262998\n",
      "Iteration  199 | Loss: 0.262998\n",
      "Iteration    0 | Loss: 0.263030\n",
      "Iteration    0 | Loss: 0.263030\n",
      "Iteration   50 | Loss: 0.262978\n",
      "Iteration   50 | Loss: 0.262978\n",
      "Iteration  100 | Loss: 0.263380\n",
      "Iteration  100 | Loss: 0.263380\n",
      "Iteration  150 | Loss: 0.263059\n",
      "Iteration  150 | Loss: 0.263059\n",
      "Iteration  199 | Loss: 0.262891\n",
      "Iteration  199 | Loss: 0.262891\n",
      "Iteration    0 | Loss: 0.262886\n",
      "Iteration    0 | Loss: 0.262886\n",
      "Iteration   50 | Loss: 0.263156\n",
      "Iteration   50 | Loss: 0.263156\n",
      "Iteration  100 | Loss: 0.262956\n",
      "Iteration  100 | Loss: 0.262956\n",
      "Iteration  150 | Loss: 0.262975\n",
      "Iteration  150 | Loss: 0.262975\n",
      "Iteration  199 | Loss: 0.262894\n",
      "Iteration  199 | Loss: 0.262894\n",
      "Iteration    0 | Loss: 0.262895\n",
      "Iteration    0 | Loss: 0.262895\n",
      "Iteration   50 | Loss: 0.262964\n",
      "Iteration   50 | Loss: 0.262964\n",
      "Iteration  100 | Loss: 0.262962\n",
      "Iteration  100 | Loss: 0.262962\n",
      "Iteration  150 | Loss: 0.262870\n",
      "Iteration  150 | Loss: 0.262870\n",
      "Iteration  199 | Loss: 0.262949\n",
      "Iteration  199 | Loss: 0.262949\n",
      "Iteration    0 | Loss: 0.262960\n",
      "Iteration    0 | Loss: 0.262960\n",
      "Iteration   50 | Loss: 0.263040\n",
      "Iteration   50 | Loss: 0.263040\n",
      "Iteration  100 | Loss: 0.262874\n",
      "Iteration  100 | Loss: 0.262874\n",
      "Iteration  150 | Loss: 0.263300\n",
      "Iteration  150 | Loss: 0.263300\n",
      "Iteration  199 | Loss: 0.262955\n",
      "Iteration  199 | Loss: 0.262955\n",
      "Iteration    0 | Loss: 0.262981\n",
      "Iteration    0 | Loss: 0.262981\n",
      "Iteration   50 | Loss: 0.262918\n",
      "Iteration   50 | Loss: 0.262918\n",
      "Iteration  100 | Loss: 0.263055\n",
      "Iteration  100 | Loss: 0.263055\n",
      "Iteration  150 | Loss: 0.263062\n",
      "Iteration  150 | Loss: 0.263062\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration    0 | Loss: 0.262925\n",
      "Iteration    0 | Loss: 0.262925\n",
      "Iteration   50 | Loss: 0.262886\n",
      "Iteration   50 | Loss: 0.262886\n",
      "Iteration  100 | Loss: 0.262963\n",
      "Iteration  100 | Loss: 0.262963\n",
      "Iteration  150 | Loss: 0.262968\n",
      "Iteration  150 | Loss: 0.262968\n",
      "Iteration  199 | Loss: 0.262918\n",
      "Iteration  199 | Loss: 0.262918\n",
      "Iteration    0 | Loss: 0.262900\n",
      "Iteration    0 | Loss: 0.262900\n",
      "Iteration   50 | Loss: 0.262922\n",
      "Iteration   50 | Loss: 0.262922\n",
      "Iteration  100 | Loss: 0.262934\n",
      "Iteration  100 | Loss: 0.262934\n",
      "Iteration  150 | Loss: 0.263055\n",
      "Iteration  150 | Loss: 0.263055\n",
      "Iteration  199 | Loss: 0.263240\n",
      "Iteration  199 | Loss: 0.263240\n",
      "Iteration    0 | Loss: 0.263234\n",
      "Iteration    0 | Loss: 0.263234\n",
      "Iteration   50 | Loss: 0.262929\n",
      "Iteration   50 | Loss: 0.262929\n",
      "Iteration  100 | Loss: 0.262953\n",
      "Iteration  100 | Loss: 0.262953\n",
      "Iteration  150 | Loss: 0.263014\n",
      "Iteration  150 | Loss: 0.263014\n",
      "Iteration  199 | Loss: 0.262924\n",
      "Iteration  199 | Loss: 0.262924\n",
      "Iteration    0 | Loss: 0.262934\n",
      "Iteration    0 | Loss: 0.262934\n",
      "Iteration   50 | Loss: 0.263065\n",
      "Iteration   50 | Loss: 0.263065\n",
      "Iteration  100 | Loss: 0.262947\n",
      "Iteration  100 | Loss: 0.262947\n",
      "Iteration  150 | Loss: 0.262886\n",
      "Iteration  150 | Loss: 0.262886\n",
      "Iteration  199 | Loss: 0.263119\n",
      "Iteration  199 | Loss: 0.263119\n",
      "Iteration    0 | Loss: 0.263080\n",
      "Iteration    0 | Loss: 0.263080\n",
      "Iteration   50 | Loss: 0.262995\n",
      "Iteration   50 | Loss: 0.262995\n",
      "Iteration  100 | Loss: 0.262969\n",
      "Iteration  100 | Loss: 0.262969\n",
      "Iteration  150 | Loss: 0.262949\n",
      "Iteration  150 | Loss: 0.262949\n",
      "Iteration  199 | Loss: 0.262979\n",
      "Iteration  199 | Loss: 0.262979\n",
      "Iteration    0 | Loss: 0.262969\n",
      "Iteration    0 | Loss: 0.262969\n",
      "Iteration   50 | Loss: 0.262992\n",
      "Iteration   50 | Loss: 0.262992\n",
      "Iteration  100 | Loss: 0.262880\n",
      "Iteration  100 | Loss: 0.262880\n",
      "Iteration  150 | Loss: 0.262874\n",
      "Iteration  150 | Loss: 0.262874\n",
      "Iteration  199 | Loss: 0.262931\n",
      "Iteration  199 | Loss: 0.262931\n",
      "Iteration    0 | Loss: 0.262942\n",
      "Iteration    0 | Loss: 0.262942\n",
      "Iteration   50 | Loss: 0.262884\n",
      "Iteration   50 | Loss: 0.262884\n",
      "Iteration  100 | Loss: 0.262922\n",
      "Iteration  100 | Loss: 0.262922\n",
      "Iteration  150 | Loss: 0.263136\n",
      "Iteration  150 | Loss: 0.263136\n",
      "Iteration  199 | Loss: 0.262983\n",
      "Iteration  199 | Loss: 0.262983\n",
      "Iteration    0 | Loss: 0.262979\n",
      "Iteration    0 | Loss: 0.262979\n",
      "Iteration   50 | Loss: 0.263088\n",
      "Iteration   50 | Loss: 0.263088\n",
      "Iteration  100 | Loss: 0.263085\n",
      "Iteration  100 | Loss: 0.263085\n",
      "Iteration  150 | Loss: 0.262884\n",
      "Iteration  150 | Loss: 0.262884\n",
      "Iteration  199 | Loss: 0.263109\n",
      "Iteration  199 | Loss: 0.263109\n",
      "Iteration    0 | Loss: 0.263089\n",
      "Iteration    0 | Loss: 0.263089\n",
      "Iteration   50 | Loss: 0.263147\n",
      "Iteration   50 | Loss: 0.263147\n",
      "Iteration  100 | Loss: 0.262990\n",
      "Iteration  100 | Loss: 0.262990\n",
      "Iteration  150 | Loss: 0.262934\n",
      "Iteration  150 | Loss: 0.262934\n",
      "Iteration  199 | Loss: 0.262898\n",
      "Iteration  199 | Loss: 0.262898\n",
      "Iteration    0 | Loss: 0.262912\n",
      "Iteration    0 | Loss: 0.262912\n",
      "Iteration   50 | Loss: 0.262914\n",
      "Iteration   50 | Loss: 0.262914\n",
      "Iteration  100 | Loss: 0.262920\n",
      "Iteration  100 | Loss: 0.262920\n",
      "Iteration  150 | Loss: 0.262907\n",
      "Iteration  150 | Loss: 0.262907\n",
      "Iteration  199 | Loss: 0.263014\n",
      "Iteration  199 | Loss: 0.263014\n",
      "Iteration    0 | Loss: 0.262983\n",
      "Iteration    0 | Loss: 0.262983\n",
      "Iteration   50 | Loss: 0.262951\n",
      "Iteration   50 | Loss: 0.262951\n",
      "Iteration  100 | Loss: 0.262964\n",
      "Iteration  100 | Loss: 0.262964\n",
      "Iteration  150 | Loss: 0.263017\n",
      "Iteration  150 | Loss: 0.263017\n",
      "Iteration  199 | Loss: 0.262967\n",
      "Iteration  199 | Loss: 0.262967\n",
      "Iteration    0 | Loss: 0.262941\n",
      "Iteration    0 | Loss: 0.262941\n",
      "Iteration   50 | Loss: 0.262974\n",
      "Iteration   50 | Loss: 0.262974\n",
      "Iteration  100 | Loss: 0.262972\n",
      "Iteration  100 | Loss: 0.262972\n",
      "Iteration  150 | Loss: 0.263113\n",
      "Iteration  150 | Loss: 0.263113\n",
      "Iteration  199 | Loss: 0.263110\n",
      "Iteration  199 | Loss: 0.263110\n",
      "Iteration    0 | Loss: 0.263091\n",
      "Iteration    0 | Loss: 0.263091\n",
      "Iteration   50 | Loss: 0.263000\n",
      "Iteration   50 | Loss: 0.263000\n",
      "Iteration  100 | Loss: 0.263271\n",
      "Iteration  100 | Loss: 0.263271\n",
      "Iteration  150 | Loss: 0.262960\n",
      "Iteration  150 | Loss: 0.262960\n",
      "Iteration  199 | Loss: 0.262905\n",
      "Iteration  199 | Loss: 0.262905\n",
      "Iteration    0 | Loss: 0.262882\n",
      "Iteration    0 | Loss: 0.262882\n",
      "Iteration   50 | Loss: 0.263090\n",
      "Iteration   50 | Loss: 0.263090\n",
      "Iteration  100 | Loss: 0.262953\n",
      "Iteration  100 | Loss: 0.262953\n",
      "Iteration  150 | Loss: 0.262985\n",
      "Iteration  150 | Loss: 0.262985\n",
      "Iteration  199 | Loss: 0.263145\n",
      "Iteration  199 | Loss: 0.263145\n",
      "Iteration    0 | Loss: 0.263201\n",
      "Iteration    0 | Loss: 0.263201\n",
      "Iteration   50 | Loss: 0.262998\n",
      "Iteration   50 | Loss: 0.262998\n",
      "Iteration  100 | Loss: 0.262868\n",
      "Iteration  100 | Loss: 0.262868\n",
      "Iteration  150 | Loss: 0.262931\n",
      "Iteration  150 | Loss: 0.262931\n",
      "Iteration  199 | Loss: 0.262919\n",
      "Iteration  199 | Loss: 0.262919\n",
      "Iteration    0 | Loss: 0.262921\n",
      "Iteration    0 | Loss: 0.262921\n",
      "Iteration   50 | Loss: 0.263631\n",
      "Iteration   50 | Loss: 0.263631\n",
      "Iteration  100 | Loss: 0.263082\n",
      "Iteration  100 | Loss: 0.263082\n",
      "Iteration  150 | Loss: 0.262990\n",
      "Iteration  150 | Loss: 0.262990\n",
      "Iteration  199 | Loss: 0.262961\n",
      "Iteration  199 | Loss: 0.262961\n",
      "Iteration    0 | Loss: 0.262957\n",
      "Iteration    0 | Loss: 0.262957\n",
      "Iteration   50 | Loss: 0.262955\n",
      "Iteration   50 | Loss: 0.262955\n",
      "Iteration  100 | Loss: 0.262923\n",
      "Iteration  100 | Loss: 0.262923\n",
      "Iteration  150 | Loss: 0.262928\n",
      "Iteration  150 | Loss: 0.262928\n",
      "Iteration  199 | Loss: 0.262923\n",
      "Iteration  199 | Loss: 0.262923\n",
      "Iteration    0 | Loss: 0.262941\n",
      "Iteration    0 | Loss: 0.262941\n",
      "Iteration   50 | Loss: 0.262994\n",
      "Iteration   50 | Loss: 0.262994\n",
      "Iteration  100 | Loss: 0.262954\n",
      "Iteration  100 | Loss: 0.262954\n",
      "Iteration  150 | Loss: 0.262916\n",
      "Iteration  150 | Loss: 0.262916\n",
      "Iteration  199 | Loss: 0.262909\n",
      "Iteration  199 | Loss: 0.262909\n",
      "Iteration    0 | Loss: 0.262906\n",
      "Iteration    0 | Loss: 0.262906\n",
      "Iteration   50 | Loss: 0.262962\n",
      "Iteration   50 | Loss: 0.262962\n",
      "Iteration  100 | Loss: 0.262924\n",
      "Iteration  100 | Loss: 0.262924\n",
      "Iteration  150 | Loss: 0.262910\n",
      "Iteration  150 | Loss: 0.262910\n",
      "Iteration  199 | Loss: 0.262943\n",
      "Iteration  199 | Loss: 0.262943\n",
      "Iteration    0 | Loss: 0.262945\n",
      "Iteration    0 | Loss: 0.262945\n",
      "Iteration   50 | Loss: 0.262878\n",
      "Iteration   50 | Loss: 0.262878\n",
      "Iteration  100 | Loss: 0.262948\n",
      "Iteration  100 | Loss: 0.262948\n",
      "Iteration  150 | Loss: 0.262892\n",
      "Iteration  150 | Loss: 0.262892\n",
      "Iteration  199 | Loss: 0.262974\n",
      "Iteration  199 | Loss: 0.262974\n",
      "Iteration    0 | Loss: 0.262946\n",
      "Iteration    0 | Loss: 0.262946\n",
      "Iteration   50 | Loss: 0.263041\n",
      "Iteration   50 | Loss: 0.263041\n",
      "Iteration  100 | Loss: 0.262970\n",
      "Iteration  100 | Loss: 0.262970\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  199 | Loss: 0.263344\n",
      "Iteration  199 | Loss: 0.263344\n",
      "Iteration    0 | Loss: 0.263302\n",
      "Iteration    0 | Loss: 0.263302\n",
      "Iteration   50 | Loss: 0.262957\n",
      "Iteration   50 | Loss: 0.262957\n",
      "Iteration  100 | Loss: 0.263159\n",
      "Iteration  100 | Loss: 0.263159\n",
      "Iteration  150 | Loss: 0.263029\n",
      "Iteration  150 | Loss: 0.263029\n",
      "Iteration  199 | Loss: 0.263141\n",
      "Iteration  199 | Loss: 0.263141\n",
      "Iteration    0 | Loss: 0.263139\n",
      "Iteration    0 | Loss: 0.263139\n",
      "Iteration   50 | Loss: 0.262949\n",
      "Iteration   50 | Loss: 0.262949\n",
      "Iteration  100 | Loss: 0.262931\n",
      "Iteration  100 | Loss: 0.262931\n",
      "Iteration  150 | Loss: 0.263210\n",
      "Iteration  150 | Loss: 0.263210\n",
      "Iteration  199 | Loss: 0.263028\n",
      "Iteration  199 | Loss: 0.263028\n",
      "Iteration    0 | Loss: 0.263068\n",
      "Iteration    0 | Loss: 0.263068\n",
      "Iteration   50 | Loss: 0.263014\n",
      "Iteration   50 | Loss: 0.263014\n",
      "Iteration  100 | Loss: 0.262987\n",
      "Iteration  100 | Loss: 0.262987\n",
      "Iteration  150 | Loss: 0.263154\n",
      "Iteration  150 | Loss: 0.263154\n",
      "Iteration  199 | Loss: 0.263074\n",
      "Iteration  199 | Loss: 0.263074\n",
      "Iteration    0 | Loss: 0.263020\n",
      "Iteration    0 | Loss: 0.263020\n",
      "Iteration   50 | Loss: 0.263031\n",
      "Iteration   50 | Loss: 0.263031\n",
      "Iteration  100 | Loss: 0.263054\n",
      "Iteration  100 | Loss: 0.263054\n",
      "Iteration  150 | Loss: 0.263082\n",
      "Iteration  150 | Loss: 0.263082\n",
      "Iteration  199 | Loss: 0.262934\n",
      "Iteration  199 | Loss: 0.262934\n",
      "Iteration    0 | Loss: 0.262905\n",
      "Iteration    0 | Loss: 0.262905\n",
      "Iteration   50 | Loss: 0.263082\n",
      "Iteration   50 | Loss: 0.263082\n",
      "Iteration  100 | Loss: 0.263209\n",
      "Iteration  100 | Loss: 0.263209\n",
      "Iteration  150 | Loss: 0.262986\n",
      "Iteration  150 | Loss: 0.262986\n",
      "Iteration  199 | Loss: 0.262958\n",
      "Iteration  199 | Loss: 0.262958\n",
      "Iteration    0 | Loss: 0.262932\n",
      "Iteration    0 | Loss: 0.262932\n",
      "Iteration   50 | Loss: 0.263104\n",
      "Iteration   50 | Loss: 0.263104\n",
      "Iteration  100 | Loss: 0.262997\n",
      "Iteration  100 | Loss: 0.262997\n",
      "Iteration  150 | Loss: 0.262926\n",
      "Iteration  150 | Loss: 0.262926\n",
      "Iteration  199 | Loss: 0.263035\n",
      "Iteration  199 | Loss: 0.263035\n",
      "Iteration    0 | Loss: 0.263017\n",
      "Iteration    0 | Loss: 0.263017\n",
      "Iteration   50 | Loss: 0.263182\n",
      "Iteration   50 | Loss: 0.263182\n",
      "Iteration  100 | Loss: 0.262916\n",
      "Iteration  100 | Loss: 0.262916\n",
      "Iteration  150 | Loss: 0.262977\n",
      "Iteration  150 | Loss: 0.262977\n",
      "Iteration  199 | Loss: 0.263014\n",
      "Iteration  199 | Loss: 0.263014\n",
      "Iteration    0 | Loss: 0.263025\n",
      "Iteration    0 | Loss: 0.263025\n",
      "Iteration   50 | Loss: 0.263106\n",
      "Iteration   50 | Loss: 0.263106\n",
      "Iteration  100 | Loss: 0.262982\n",
      "Iteration  100 | Loss: 0.262982\n",
      "Iteration  150 | Loss: 0.263301\n",
      "Iteration  150 | Loss: 0.263301\n",
      "Iteration  199 | Loss: 0.263072\n",
      "Iteration  199 | Loss: 0.263072\n",
      "Iteration    0 | Loss: 0.263053\n",
      "Iteration    0 | Loss: 0.263053\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration  100 | Loss: 0.262960\n",
      "Iteration  100 | Loss: 0.262960\n",
      "Iteration  150 | Loss: 0.263034\n",
      "Iteration  150 | Loss: 0.263034\n",
      "Iteration  199 | Loss: 0.262930\n",
      "Iteration  199 | Loss: 0.262930\n",
      "Iteration    0 | Loss: 0.262936\n",
      "Iteration    0 | Loss: 0.262936\n",
      "Iteration   50 | Loss: 0.262943\n",
      "Iteration   50 | Loss: 0.262943\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  150 | Loss: 0.262885\n",
      "Iteration  150 | Loss: 0.262885\n",
      "Iteration  199 | Loss: 0.262945\n",
      "Iteration  199 | Loss: 0.262945\n",
      "Iteration    0 | Loss: 0.262926\n",
      "Iteration    0 | Loss: 0.262926\n",
      "Iteration   50 | Loss: 0.263028\n",
      "Iteration   50 | Loss: 0.263028\n",
      "Iteration  100 | Loss: 0.263177\n",
      "Iteration  100 | Loss: 0.263177\n",
      "Iteration  150 | Loss: 0.263086\n",
      "Iteration  150 | Loss: 0.263086\n",
      "Iteration  199 | Loss: 0.263017\n",
      "Iteration  199 | Loss: 0.263017\n",
      "Iteration    0 | Loss: 0.263064\n",
      "Iteration    0 | Loss: 0.263064\n",
      "Iteration   50 | Loss: 0.263039\n",
      "Iteration   50 | Loss: 0.263039\n",
      "Iteration  100 | Loss: 0.262938\n",
      "Iteration  100 | Loss: 0.262938\n",
      "Iteration  150 | Loss: 0.263018\n",
      "Iteration  150 | Loss: 0.263018\n",
      "Iteration  199 | Loss: 0.263243\n",
      "Iteration  199 | Loss: 0.263243\n",
      "Iteration    0 | Loss: 0.263268\n",
      "Iteration    0 | Loss: 0.263268\n",
      "Iteration   50 | Loss: 0.263174\n",
      "Iteration   50 | Loss: 0.263174\n",
      "Iteration  100 | Loss: 0.262932\n",
      "Iteration  100 | Loss: 0.262932\n",
      "Iteration  150 | Loss: 0.262977\n",
      "Iteration  150 | Loss: 0.262977\n",
      "Iteration  199 | Loss: 0.262975\n",
      "Iteration  199 | Loss: 0.262975\n",
      "Iteration    0 | Loss: 0.262934\n",
      "Iteration    0 | Loss: 0.262934\n",
      "Iteration   50 | Loss: 0.263316\n",
      "Iteration   50 | Loss: 0.263316\n",
      "Iteration  100 | Loss: 0.263263\n",
      "Iteration  100 | Loss: 0.263263\n",
      "Iteration  150 | Loss: 0.263583\n",
      "Iteration  150 | Loss: 0.263583\n",
      "Iteration  199 | Loss: 0.263166\n",
      "Iteration  199 | Loss: 0.263166\n",
      "Iteration    0 | Loss: 0.263139\n",
      "Iteration    0 | Loss: 0.263139\n",
      "Iteration   50 | Loss: 0.262977\n",
      "Iteration   50 | Loss: 0.262977\n",
      "Iteration  100 | Loss: 0.263063\n",
      "Iteration  100 | Loss: 0.263063\n",
      "Iteration  150 | Loss: 0.262926\n",
      "Iteration  150 | Loss: 0.262926\n",
      "Iteration  199 | Loss: 0.262966\n",
      "Iteration  199 | Loss: 0.262966\n",
      "Iteration    0 | Loss: 0.263005\n",
      "Iteration    0 | Loss: 0.263005\n",
      "Iteration   50 | Loss: 0.262975\n",
      "Iteration   50 | Loss: 0.262975\n",
      "Iteration  100 | Loss: 0.263075\n",
      "Iteration  100 | Loss: 0.263075\n",
      "Iteration  150 | Loss: 0.263101\n",
      "Iteration  150 | Loss: 0.263101\n",
      "Iteration  199 | Loss: 0.263091\n",
      "Iteration  199 | Loss: 0.263091\n",
      "Iteration    0 | Loss: 0.263087\n",
      "Iteration    0 | Loss: 0.263087\n",
      "Iteration   50 | Loss: 0.263218\n",
      "Iteration   50 | Loss: 0.263218\n",
      "Iteration  100 | Loss: 0.263061\n",
      "Iteration  100 | Loss: 0.263061\n",
      "Iteration  150 | Loss: 0.262895\n",
      "Iteration  150 | Loss: 0.262895\n",
      "Iteration  199 | Loss: 0.262935\n",
      "Iteration  199 | Loss: 0.262935\n",
      "Iteration    0 | Loss: 0.262961\n",
      "Iteration    0 | Loss: 0.262961\n",
      "Iteration   50 | Loss: 0.262948\n",
      "Iteration   50 | Loss: 0.262948\n",
      "Iteration  100 | Loss: 0.262918\n",
      "Iteration  100 | Loss: 0.262918\n",
      "Iteration  150 | Loss: 0.262990\n",
      "Iteration  150 | Loss: 0.262990\n",
      "Iteration  199 | Loss: 0.263120\n",
      "Iteration  199 | Loss: 0.263120\n",
      "Iteration    0 | Loss: 0.263095\n",
      "Iteration    0 | Loss: 0.263095\n",
      "Iteration   50 | Loss: 0.262882\n",
      "Iteration   50 | Loss: 0.262882\n",
      "Iteration  100 | Loss: 0.263108\n",
      "Iteration  100 | Loss: 0.263108\n",
      "Iteration  150 | Loss: 0.262915\n",
      "Iteration  150 | Loss: 0.262915\n",
      "Iteration  199 | Loss: 0.262866\n",
      "Iteration  199 | Loss: 0.262866\n",
      "Iteration    0 | Loss: 0.262874\n",
      "Iteration    0 | Loss: 0.262874\n",
      "Iteration   50 | Loss: 0.262906\n",
      "Iteration   50 | Loss: 0.262906\n",
      "Iteration  100 | Loss: 0.262909\n",
      "Iteration  100 | Loss: 0.262909\n",
      "Iteration  150 | Loss: 0.262886\n",
      "Iteration  150 | Loss: 0.262886\n",
      "Iteration  199 | Loss: 0.262916\n",
      "Iteration  199 | Loss: 0.262916\n",
      "Iteration    0 | Loss: 0.262903\n",
      "Iteration    0 | Loss: 0.262903\n",
      "Iteration   50 | Loss: 0.262927\n",
      "Iteration   50 | Loss: 0.262927\n",
      "Iteration  100 | Loss: 0.263197\n",
      "Iteration  100 | Loss: 0.263197\n",
      "Iteration  150 | Loss: 0.263037\n",
      "Iteration  150 | Loss: 0.263037\n",
      "Iteration  199 | Loss: 0.262964\n",
      "Iteration  199 | Loss: 0.262964\n",
      "Iteration    0 | Loss: 0.262974\n",
      "Iteration    0 | Loss: 0.262974\n",
      "Iteration   50 | Loss: 0.262967\n",
      "Iteration   50 | Loss: 0.262967\n",
      "Iteration  100 | Loss: 0.262974\n",
      "Iteration  100 | Loss: 0.262974\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  199 | Loss: 0.262988\n",
      "Iteration  199 | Loss: 0.262988\n",
      "Iteration    0 | Loss: 0.262969\n",
      "Iteration    0 | Loss: 0.262969\n",
      "Iteration   50 | Loss: 0.262911\n",
      "Iteration   50 | Loss: 0.262911\n",
      "Iteration  100 | Loss: 0.263077\n",
      "Iteration  100 | Loss: 0.263077\n",
      "Iteration  150 | Loss: 0.263064\n",
      "Iteration  150 | Loss: 0.263064\n",
      "Iteration  199 | Loss: 0.263087\n",
      "Iteration  199 | Loss: 0.263087\n",
      "Iteration    0 | Loss: 0.263072\n",
      "Iteration    0 | Loss: 0.263072\n",
      "Iteration   50 | Loss: 0.262897\n",
      "Iteration   50 | Loss: 0.262897\n",
      "Iteration  100 | Loss: 0.263078\n",
      "Iteration  100 | Loss: 0.263078\n",
      "Iteration  150 | Loss: 0.262970\n",
      "Iteration  150 | Loss: 0.262970\n",
      "Iteration  199 | Loss: 0.263000\n",
      "Iteration  199 | Loss: 0.263000\n",
      "Iteration    0 | Loss: 0.262980\n",
      "Iteration    0 | Loss: 0.262980\n",
      "Iteration   50 | Loss: 0.262881\n",
      "Iteration   50 | Loss: 0.262881\n",
      "Iteration  100 | Loss: 0.263102\n",
      "Iteration  100 | Loss: 0.263102\n",
      "Iteration  150 | Loss: 0.263279\n",
      "Iteration  150 | Loss: 0.263279\n",
      "Iteration  199 | Loss: 0.262915\n",
      "Iteration  199 | Loss: 0.262915\n",
      "Iteration    0 | Loss: 0.262907\n",
      "Iteration    0 | Loss: 0.262907\n",
      "Iteration   50 | Loss: 0.262946\n",
      "Iteration   50 | Loss: 0.262946\n",
      "Iteration  100 | Loss: 0.262993\n",
      "Iteration  100 | Loss: 0.262993\n",
      "Iteration  150 | Loss: 0.263127\n",
      "Iteration  150 | Loss: 0.263127\n",
      "Iteration  199 | Loss: 0.262982\n",
      "Iteration  199 | Loss: 0.262982\n",
      "Iteration    0 | Loss: 0.262975\n",
      "Iteration    0 | Loss: 0.262975\n",
      "Iteration   50 | Loss: 0.262943\n",
      "Iteration   50 | Loss: 0.262943\n",
      "Iteration  100 | Loss: 0.262874\n",
      "Iteration  100 | Loss: 0.262874\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  199 | Loss: 0.262947\n",
      "Iteration  199 | Loss: 0.262947\n",
      "Iteration    0 | Loss: 0.262941\n",
      "Iteration    0 | Loss: 0.262941\n",
      "Iteration   50 | Loss: 0.262974\n",
      "Iteration   50 | Loss: 0.262974\n",
      "Iteration  100 | Loss: 0.262941\n",
      "Iteration  100 | Loss: 0.262941\n",
      "Iteration  150 | Loss: 0.262959\n",
      "Iteration  150 | Loss: 0.262959\n",
      "Iteration  199 | Loss: 0.263104\n",
      "Iteration  199 | Loss: 0.263104\n",
      "Iteration    0 | Loss: 0.263105\n",
      "Iteration    0 | Loss: 0.263105\n",
      "Iteration   50 | Loss: 0.263533\n",
      "Iteration   50 | Loss: 0.263533\n",
      "Iteration  100 | Loss: 0.262994\n",
      "Iteration  100 | Loss: 0.262994\n",
      "Iteration  150 | Loss: 0.263041\n",
      "Iteration  150 | Loss: 0.263041\n",
      "Iteration  199 | Loss: 0.262943\n",
      "Iteration  199 | Loss: 0.262943\n",
      "Iteration    0 | Loss: 0.262943\n",
      "Iteration    0 | Loss: 0.262943\n",
      "Iteration   50 | Loss: 0.262875\n",
      "Iteration   50 | Loss: 0.262875\n",
      "Iteration  100 | Loss: 0.263297\n",
      "Iteration  100 | Loss: 0.263297\n",
      "Iteration  150 | Loss: 0.262991\n",
      "Iteration  150 | Loss: 0.262991\n",
      "Iteration  199 | Loss: 0.262946\n",
      "Iteration  199 | Loss: 0.262946\n",
      "Iteration    0 | Loss: 0.262984\n",
      "Iteration    0 | Loss: 0.262984\n",
      "Iteration   50 | Loss: 0.262977\n",
      "Iteration   50 | Loss: 0.262977\n",
      "Iteration  100 | Loss: 0.262920\n",
      "Iteration  100 | Loss: 0.262920\n",
      "Iteration  150 | Loss: 0.263049\n",
      "Iteration  150 | Loss: 0.263049\n",
      "Iteration  199 | Loss: 0.263122\n",
      "Iteration  199 | Loss: 0.263122\n",
      "Iteration    0 | Loss: 0.263148\n",
      "Iteration    0 | Loss: 0.263148\n",
      "Iteration   50 | Loss: 0.263014\n",
      "Iteration   50 | Loss: 0.263014\n",
      "Iteration  100 | Loss: 0.263170\n",
      "Iteration  100 | Loss: 0.263170\n",
      "Iteration  150 | Loss: 0.263029\n",
      "Iteration  150 | Loss: 0.263029\n",
      "Iteration  199 | Loss: 0.262977\n",
      "Iteration  199 | Loss: 0.262977\n",
      "Iteration    0 | Loss: 0.263018\n",
      "Iteration    0 | Loss: 0.263018\n",
      "Iteration   50 | Loss: 0.262912\n",
      "Iteration   50 | Loss: 0.262912\n",
      "Iteration  100 | Loss: 0.262955\n",
      "Iteration  100 | Loss: 0.262955\n",
      "Iteration  150 | Loss: 0.262956\n",
      "Iteration  150 | Loss: 0.262956\n",
      "Iteration  199 | Loss: 0.262900\n",
      "Iteration  199 | Loss: 0.262900\n",
      "Iteration    0 | Loss: 0.262905\n",
      "Iteration    0 | Loss: 0.262905\n",
      "Iteration   50 | Loss: 0.262905\n",
      "Iteration   50 | Loss: 0.262905\n",
      "Iteration  100 | Loss: 0.263042\n",
      "Iteration  100 | Loss: 0.263042\n",
      "Iteration  150 | Loss: 0.263129\n",
      "Iteration  150 | Loss: 0.263129\n",
      "Iteration  199 | Loss: 0.262985\n",
      "Iteration  199 | Loss: 0.262985\n",
      "Iteration    0 | Loss: 0.262947\n",
      "Iteration    0 | Loss: 0.262947\n",
      "Iteration   50 | Loss: 0.263026\n",
      "Iteration   50 | Loss: 0.263026\n",
      "Iteration  100 | Loss: 0.262935\n",
      "Iteration  100 | Loss: 0.262935\n",
      "Iteration  150 | Loss: 0.263024\n",
      "Iteration  150 | Loss: 0.263024\n",
      "Iteration  199 | Loss: 0.262968\n",
      "Iteration  199 | Loss: 0.262968\n",
      "Iteration    0 | Loss: 0.262976\n",
      "Iteration    0 | Loss: 0.262976\n",
      "Iteration   50 | Loss: 0.262952\n",
      "Iteration   50 | Loss: 0.262952\n",
      "Iteration  100 | Loss: 0.262896\n",
      "Iteration  100 | Loss: 0.262896\n",
      "Iteration  150 | Loss: 0.262919\n",
      "Iteration  150 | Loss: 0.262919\n",
      "Iteration  199 | Loss: 0.263086\n",
      "Iteration  199 | Loss: 0.263086\n",
      "Iteration    0 | Loss: 0.263024\n",
      "Iteration    0 | Loss: 0.263024\n",
      "Iteration   50 | Loss: 0.262960\n",
      "Iteration   50 | Loss: 0.262960\n",
      "Iteration  100 | Loss: 0.262909\n",
      "Iteration  100 | Loss: 0.262909\n",
      "Iteration  150 | Loss: 0.262913\n",
      "Iteration  150 | Loss: 0.262913\n",
      "Iteration  199 | Loss: 0.262898\n",
      "Iteration  199 | Loss: 0.262898\n",
      "Iteration    0 | Loss: 0.262907\n",
      "Iteration    0 | Loss: 0.262907\n",
      "Iteration   50 | Loss: 0.262918\n",
      "Iteration   50 | Loss: 0.262918\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  100 | Loss: 0.262876\n",
      "Iteration  150 | Loss: 0.262916\n",
      "Iteration  150 | Loss: 0.262916\n",
      "Iteration  199 | Loss: 0.262937\n",
      "Iteration  199 | Loss: 0.262937\n",
      "Iteration    0 | Loss: 0.262929\n",
      "Iteration    0 | Loss: 0.262929\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration  100 | Loss: 0.263021\n",
      "Iteration  100 | Loss: 0.263021\n",
      "Iteration  150 | Loss: 0.263042\n",
      "Iteration  150 | Loss: 0.263042\n",
      "Iteration  199 | Loss: 0.263001\n",
      "Iteration  199 | Loss: 0.263001\n",
      "Iteration    0 | Loss: 0.263030\n",
      "Iteration    0 | Loss: 0.263030\n",
      "Iteration   50 | Loss: 0.262882\n",
      "Iteration   50 | Loss: 0.262882\n",
      "Iteration  100 | Loss: 0.262884\n",
      "Iteration  100 | Loss: 0.262884\n",
      "Iteration  150 | Loss: 0.262871\n",
      "Iteration  150 | Loss: 0.262871\n",
      "Iteration  199 | Loss: 0.262958\n",
      "Iteration  199 | Loss: 0.262958\n",
      "Iteration    0 | Loss: 0.262935\n",
      "Iteration    0 | Loss: 0.262935\n",
      "Iteration   50 | Loss: 0.262939\n",
      "Iteration   50 | Loss: 0.262939\n",
      "Iteration  100 | Loss: 0.262914\n",
      "Iteration  100 | Loss: 0.262914\n",
      "Iteration  150 | Loss: 0.263015\n",
      "Iteration  150 | Loss: 0.263015\n",
      "Iteration  199 | Loss: 0.262911\n",
      "Iteration  199 | Loss: 0.262911\n",
      "Iteration    0 | Loss: 0.262897\n",
      "Iteration    0 | Loss: 0.262897\n",
      "Iteration   50 | Loss: 0.263176\n",
      "Iteration   50 | Loss: 0.263176\n",
      "Iteration  100 | Loss: 0.263175\n",
      "Iteration  100 | Loss: 0.263175\n",
      "Iteration  150 | Loss: 0.263045\n",
      "Iteration  150 | Loss: 0.263045\n",
      "Iteration  199 | Loss: 0.263000\n",
      "Iteration  199 | Loss: 0.263000\n",
      "Iteration    0 | Loss: 0.263061\n",
      "Iteration    0 | Loss: 0.263061\n",
      "Iteration   50 | Loss: 0.262917\n",
      "Iteration   50 | Loss: 0.262917\n",
      "Iteration  100 | Loss: 0.262917\n",
      "Iteration  100 | Loss: 0.262917\n",
      "Iteration  150 | Loss: 0.262944\n",
      "Iteration  150 | Loss: 0.262944\n",
      "Iteration  199 | Loss: 0.263057\n",
      "Iteration  199 | Loss: 0.263057\n",
      "Iteration    0 | Loss: 0.263044\n",
      "Iteration    0 | Loss: 0.263044\n",
      "Iteration   50 | Loss: 0.262972\n",
      "Iteration   50 | Loss: 0.262972\n",
      "Iteration  100 | Loss: 0.263068\n",
      "Iteration  100 | Loss: 0.263068\n",
      "Iteration  150 | Loss: 0.263148\n",
      "Iteration  150 | Loss: 0.263148\n",
      "Iteration  199 | Loss: 0.263010\n",
      "Iteration  199 | Loss: 0.263010\n",
      "Iteration    0 | Loss: 0.263087\n",
      "Iteration    0 | Loss: 0.263087\n",
      "Iteration   50 | Loss: 0.262929\n",
      "Iteration   50 | Loss: 0.262929\n",
      "Iteration  100 | Loss: 0.262927\n",
      "Iteration  100 | Loss: 0.262927\n",
      "Iteration  150 | Loss: 0.262929\n",
      "Iteration  150 | Loss: 0.262929\n",
      "Iteration  199 | Loss: 0.262907\n",
      "Iteration  199 | Loss: 0.262907\n",
      "Iteration    0 | Loss: 0.262923\n",
      "Iteration    0 | Loss: 0.262923\n",
      "Iteration   50 | Loss: 0.263062\n",
      "Iteration   50 | Loss: 0.263062\n",
      "Iteration  100 | Loss: 0.263062\n",
      "Iteration  100 | Loss: 0.263062\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  199 | Loss: 0.263130\n",
      "Iteration  199 | Loss: 0.263130\n",
      "Iteration    0 | Loss: 0.263116\n",
      "Iteration    0 | Loss: 0.263116\n",
      "Iteration   50 | Loss: 0.262945\n",
      "Iteration   50 | Loss: 0.262945\n",
      "Iteration  100 | Loss: 0.262993\n",
      "Iteration  100 | Loss: 0.262993\n",
      "Iteration  150 | Loss: 0.263088\n",
      "Iteration  150 | Loss: 0.263088\n",
      "Iteration  199 | Loss: 0.263045\n",
      "Iteration  199 | Loss: 0.263045\n",
      "Iteration    0 | Loss: 0.263072\n",
      "Iteration    0 | Loss: 0.263072\n",
      "Iteration   50 | Loss: 0.262992\n",
      "Iteration   50 | Loss: 0.262992\n",
      "Iteration  100 | Loss: 0.262942\n",
      "Iteration  100 | Loss: 0.262942\n",
      "Iteration  150 | Loss: 0.263045\n",
      "Iteration  150 | Loss: 0.263045\n",
      "Iteration  199 | Loss: 0.262912\n",
      "Iteration  199 | Loss: 0.262912\n",
      "Iteration    0 | Loss: 0.262954\n",
      "Iteration    0 | Loss: 0.262954\n",
      "Iteration   50 | Loss: 0.262975\n",
      "Iteration   50 | Loss: 0.262975\n",
      "Iteration  100 | Loss: 0.262972\n",
      "Iteration  100 | Loss: 0.262972\n",
      "Iteration  150 | Loss: 0.262971\n",
      "Iteration  150 | Loss: 0.262971\n",
      "Iteration  199 | Loss: 0.263163\n",
      "Iteration  199 | Loss: 0.263163\n",
      "Iteration    0 | Loss: 0.263132\n",
      "Iteration    0 | Loss: 0.263132\n",
      "Iteration   50 | Loss: 0.262994\n",
      "Iteration   50 | Loss: 0.262994\n",
      "Iteration  100 | Loss: 0.262875\n",
      "Iteration  100 | Loss: 0.262875\n",
      "Iteration  150 | Loss: 0.262960\n",
      "Iteration  150 | Loss: 0.262960\n",
      "Iteration  199 | Loss: 0.262891\n",
      "Iteration  199 | Loss: 0.262891\n",
      "Iteration    0 | Loss: 0.262883\n",
      "Iteration    0 | Loss: 0.262883\n",
      "Iteration   50 | Loss: 0.262972\n",
      "Iteration   50 | Loss: 0.262972\n",
      "Iteration  100 | Loss: 0.263011\n",
      "Iteration  100 | Loss: 0.263011\n",
      "Iteration  150 | Loss: 0.263157\n",
      "Iteration  150 | Loss: 0.263157\n",
      "Iteration  199 | Loss: 0.263043\n",
      "Iteration  199 | Loss: 0.263043\n",
      "Iteration    0 | Loss: 0.263023\n",
      "Iteration    0 | Loss: 0.263023\n",
      "Iteration   50 | Loss: 0.262891\n",
      "Iteration   50 | Loss: 0.262891\n",
      "Iteration  100 | Loss: 0.262988\n",
      "Iteration  100 | Loss: 0.262988\n",
      "Iteration  150 | Loss: 0.263062\n",
      "Iteration  150 | Loss: 0.263062\n",
      "Iteration  199 | Loss: 0.263035\n",
      "Iteration  199 | Loss: 0.263035\n",
      "Iteration    0 | Loss: 0.263050\n",
      "Iteration    0 | Loss: 0.263050\n",
      "Iteration   50 | Loss: 0.263010\n",
      "Iteration   50 | Loss: 0.263010\n",
      "Iteration  100 | Loss: 0.263191\n",
      "Iteration  100 | Loss: 0.263191\n",
      "Iteration  150 | Loss: 0.262986\n",
      "Iteration  150 | Loss: 0.262986\n",
      "Iteration  199 | Loss: 0.263073\n",
      "Iteration  199 | Loss: 0.263073\n",
      "Iteration    0 | Loss: 0.263071\n",
      "Iteration    0 | Loss: 0.263071\n",
      "Iteration   50 | Loss: 0.262963\n",
      "Iteration   50 | Loss: 0.262963\n",
      "Iteration  100 | Loss: 0.262890\n",
      "Iteration  100 | Loss: 0.262890\n",
      "Iteration  150 | Loss: 0.263242\n",
      "Iteration  150 | Loss: 0.263242\n",
      "Iteration  199 | Loss: 0.262986\n",
      "Iteration  199 | Loss: 0.262986\n",
      "Iteration    0 | Loss: 0.263039\n",
      "Iteration    0 | Loss: 0.263039\n",
      "Iteration   50 | Loss: 0.263077\n",
      "Iteration   50 | Loss: 0.263077\n",
      "Iteration  100 | Loss: 0.263035\n",
      "Iteration  100 | Loss: 0.263035\n",
      "Iteration  150 | Loss: 0.263197\n",
      "Iteration  150 | Loss: 0.263197\n",
      "Iteration  199 | Loss: 0.262991\n",
      "Iteration  199 | Loss: 0.262991\n",
      "Iteration    0 | Loss: 0.263003\n",
      "Iteration    0 | Loss: 0.263003\n",
      "Iteration   50 | Loss: 0.262892\n",
      "Iteration   50 | Loss: 0.262892\n",
      "Iteration  100 | Loss: 0.263146\n",
      "Iteration  100 | Loss: 0.263146\n",
      "Iteration  150 | Loss: 0.262910\n",
      "Iteration  150 | Loss: 0.262910\n",
      "Iteration  199 | Loss: 0.262925\n",
      "Iteration  199 | Loss: 0.262925\n",
      "Iteration    0 | Loss: 0.262956\n",
      "Iteration    0 | Loss: 0.262956\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration   50 | Loss: 0.262989\n",
      "Iteration  100 | Loss: 0.263207\n",
      "Iteration  100 | Loss: 0.263207\n",
      "Iteration  150 | Loss: 0.262902\n",
      "Iteration  150 | Loss: 0.262902\n",
      "Iteration  199 | Loss: 0.263138\n",
      "Iteration  199 | Loss: 0.263138\n",
      "Iteration    0 | Loss: 0.263121\n",
      "Iteration    0 | Loss: 0.263121\n",
      "Iteration   50 | Loss: 0.262951\n",
      "Iteration   50 | Loss: 0.262951\n",
      "Iteration  100 | Loss: 0.262967\n",
      "Iteration  100 | Loss: 0.262967\n",
      "Iteration  150 | Loss: 0.263166\n",
      "Iteration  150 | Loss: 0.263166\n",
      "Iteration  199 | Loss: 0.263074\n",
      "Iteration  199 | Loss: 0.263074\n",
      "Iteration    0 | Loss: 0.263013\n",
      "Iteration    0 | Loss: 0.263013\n",
      "Iteration   50 | Loss: 0.262933\n",
      "Iteration   50 | Loss: 0.262933\n",
      "Iteration  100 | Loss: 0.263133\n",
      "Iteration  100 | Loss: 0.263133\n",
      "Iteration  150 | Loss: 0.262913\n",
      "Iteration  150 | Loss: 0.262913\n",
      "Iteration  199 | Loss: 0.262978\n",
      "Iteration  199 | Loss: 0.262978\n",
      "Iteration    0 | Loss: 0.263000\n",
      "Iteration    0 | Loss: 0.263000\n",
      "Iteration   50 | Loss: 0.262873\n",
      "Iteration   50 | Loss: 0.262873\n",
      "Iteration  100 | Loss: 0.262965\n",
      "Iteration  100 | Loss: 0.262965\n",
      "Iteration  150 | Loss: 0.263120\n",
      "Iteration  150 | Loss: 0.263120\n",
      "Iteration  199 | Loss: 0.262988\n",
      "Iteration  199 | Loss: 0.262988\n",
      "Iteration    0 | Loss: 0.262977\n",
      "Iteration    0 | Loss: 0.262977\n",
      "Iteration   50 | Loss: 0.263002\n",
      "Iteration   50 | Loss: 0.263002\n",
      "Iteration  100 | Loss: 0.263102\n",
      "Iteration  100 | Loss: 0.263102\n",
      "Iteration  150 | Loss: 0.263270\n",
      "Iteration  150 | Loss: 0.263270\n",
      "Iteration  199 | Loss: 0.263029\n",
      "Iteration  199 | Loss: 0.263029\n",
      "Iteration    0 | Loss: 0.263045\n",
      "Iteration    0 | Loss: 0.263045\n",
      "Iteration   50 | Loss: 0.263071\n",
      "Iteration   50 | Loss: 0.263071\n",
      "Iteration  100 | Loss: 0.262967\n",
      "Iteration  100 | Loss: 0.262967\n",
      "Iteration  150 | Loss: 0.263055\n",
      "Iteration  150 | Loss: 0.263055\n",
      "Iteration  199 | Loss: 0.263096\n",
      "Iteration  199 | Loss: 0.263096\n",
      "Iteration    0 | Loss: 0.263071\n",
      "Iteration    0 | Loss: 0.263071\n",
      "Iteration   50 | Loss: 0.263030\n",
      "Iteration   50 | Loss: 0.263030\n",
      "Iteration  100 | Loss: 0.263331\n",
      "Iteration  100 | Loss: 0.263331\n",
      "Iteration  150 | Loss: 0.263068\n",
      "Iteration  150 | Loss: 0.263068\n",
      "Iteration  199 | Loss: 0.263109\n",
      "Iteration  199 | Loss: 0.263109\n",
      "Iteration    0 | Loss: 0.263087\n",
      "Iteration    0 | Loss: 0.263087\n",
      "Iteration   50 | Loss: 0.263079\n",
      "Iteration   50 | Loss: 0.263079\n",
      "Iteration  100 | Loss: 0.262987\n",
      "Iteration  100 | Loss: 0.262987\n",
      "Iteration  150 | Loss: 0.263065\n",
      "Iteration  150 | Loss: 0.263065\n",
      "Iteration  199 | Loss: 0.263111\n",
      "Iteration  199 | Loss: 0.263111\n",
      "Iteration    0 | Loss: 0.263107\n",
      "Iteration    0 | Loss: 0.263107\n",
      "Iteration   50 | Loss: 0.262988\n",
      "Iteration   50 | Loss: 0.262988\n",
      "Iteration  100 | Loss: 0.263024\n",
      "Iteration  100 | Loss: 0.263024\n",
      "Iteration  150 | Loss: 0.263428\n",
      "Iteration  150 | Loss: 0.263428\n",
      "Iteration  199 | Loss: 0.263573\n",
      "Iteration  199 | Loss: 0.263573\n",
      "Iteration    0 | Loss: 0.263512\n",
      "Iteration    0 | Loss: 0.263512\n",
      "Iteration   50 | Loss: 0.262994\n",
      "Iteration   50 | Loss: 0.262994\n",
      "Iteration  100 | Loss: 0.262877\n",
      "Iteration  100 | Loss: 0.262877\n",
      "Iteration  150 | Loss: 0.262886\n",
      "Iteration  150 | Loss: 0.262886\n",
      "Iteration  199 | Loss: 0.262942\n",
      "Iteration  199 | Loss: 0.262942\n",
      "Iteration    0 | Loss: 0.262951\n",
      "Iteration    0 | Loss: 0.262951\n",
      "Iteration   50 | Loss: 0.262894\n",
      "Iteration   50 | Loss: 0.262894\n",
      "Iteration  100 | Loss: 0.262966\n",
      "Iteration  100 | Loss: 0.262966\n",
      "Iteration  150 | Loss: 0.263178\n",
      "Iteration  150 | Loss: 0.263178\n",
      "Iteration  199 | Loss: 0.262870\n",
      "Iteration  199 | Loss: 0.262870\n",
      "Iteration    0 | Loss: 0.262883\n",
      "Iteration    0 | Loss: 0.262883\n",
      "Iteration   50 | Loss: 0.262919\n",
      "Iteration   50 | Loss: 0.262919\n",
      "Iteration  100 | Loss: 0.262880\n",
      "Iteration  100 | Loss: 0.262880\n",
      "Iteration  150 | Loss: 0.263031\n",
      "Iteration  150 | Loss: 0.263031\n",
      "Iteration  199 | Loss: 0.262973\n",
      "Iteration  199 | Loss: 0.262973\n",
      "Iteration    0 | Loss: 0.262935\n",
      "Iteration    0 | Loss: 0.262935\n",
      "Iteration   50 | Loss: 0.263042\n",
      "Iteration   50 | Loss: 0.263042\n",
      "Iteration  100 | Loss: 0.263177\n",
      "Iteration  100 | Loss: 0.263177\n",
      "Iteration  150 | Loss: 0.262912\n",
      "Iteration  150 | Loss: 0.262912\n",
      "Iteration  199 | Loss: 0.263013\n",
      "Iteration  199 | Loss: 0.263013\n",
      "Iteration    0 | Loss: 0.263007\n",
      "Iteration    0 | Loss: 0.263007\n",
      "Iteration   50 | Loss: 0.263050\n",
      "Iteration   50 | Loss: 0.263050\n",
      "Iteration  100 | Loss: 0.262922\n",
      "Iteration  100 | Loss: 0.262922\n",
      "Iteration  150 | Loss: 0.262856\n",
      "Iteration  150 | Loss: 0.262856\n",
      "Iteration  199 | Loss: 0.262968\n",
      "Iteration  199 | Loss: 0.262968\n",
      "Iteration    0 | Loss: 0.262948\n",
      "Iteration    0 | Loss: 0.262948\n",
      "Iteration   50 | Loss: 0.262890\n",
      "Iteration   50 | Loss: 0.262890\n",
      "Iteration  100 | Loss: 0.262919\n",
      "Iteration  100 | Loss: 0.262919\n",
      "Iteration  150 | Loss: 0.262914\n",
      "Iteration  150 | Loss: 0.262914\n",
      "Iteration  199 | Loss: 0.262996\n",
      "Iteration  199 | Loss: 0.262996\n",
      "Iteration    0 | Loss: 0.263040\n",
      "Iteration    0 | Loss: 0.263040\n",
      "Iteration   50 | Loss: 0.262999\n",
      "Iteration   50 | Loss: 0.262999\n",
      "Iteration  100 | Loss: 0.262921\n",
      "Iteration  100 | Loss: 0.262921\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  199 | Loss: 0.263072\n",
      "Iteration  199 | Loss: 0.263072\n",
      "Iteration    0 | Loss: 0.263089\n",
      "Iteration    0 | Loss: 0.263089\n",
      "Iteration   50 | Loss: 0.262913\n",
      "Iteration   50 | Loss: 0.262913\n",
      "Iteration  100 | Loss: 0.262994\n",
      "Iteration  100 | Loss: 0.262994\n",
      "Iteration  150 | Loss: 0.262873\n",
      "Iteration  150 | Loss: 0.262873\n",
      "Iteration  199 | Loss: 0.263073\n",
      "Iteration  199 | Loss: 0.263073\n",
      "Iteration    0 | Loss: 0.263064\n",
      "Iteration    0 | Loss: 0.263064\n",
      "Iteration   50 | Loss: 0.262892\n",
      "Iteration   50 | Loss: 0.262892\n",
      "Iteration  100 | Loss: 0.263026\n",
      "Iteration  100 | Loss: 0.263026\n",
      "Iteration  150 | Loss: 0.262932\n",
      "Iteration  150 | Loss: 0.262932\n",
      "Iteration  199 | Loss: 0.262940\n",
      "Iteration  199 | Loss: 0.262940\n",
      "Iteration    0 | Loss: 0.262980\n",
      "Iteration    0 | Loss: 0.262980\n",
      "Iteration   50 | Loss: 0.263094\n",
      "Iteration   50 | Loss: 0.263094\n",
      "Iteration  100 | Loss: 0.262972\n",
      "Iteration  100 | Loss: 0.262972\n",
      "Iteration  150 | Loss: 0.262921\n",
      "Iteration  150 | Loss: 0.262921\n",
      "Iteration  199 | Loss: 0.262879\n",
      "Iteration  199 | Loss: 0.262879\n",
      "Iteration    0 | Loss: 0.262885\n",
      "Iteration    0 | Loss: 0.262885\n",
      "Iteration   50 | Loss: 0.262903\n",
      "Iteration   50 | Loss: 0.262903\n",
      "Iteration  100 | Loss: 0.262927\n",
      "Iteration  100 | Loss: 0.262927\n",
      "Iteration  150 | Loss: 0.262956\n",
      "Iteration  150 | Loss: 0.262956\n",
      "Iteration  199 | Loss: 0.262995\n",
      "Iteration  199 | Loss: 0.262995\n",
      "Iteration    0 | Loss: 0.262950\n",
      "Iteration    0 | Loss: 0.262950\n",
      "Iteration   50 | Loss: 0.262946\n",
      "Iteration   50 | Loss: 0.262946\n",
      "Iteration  100 | Loss: 0.262974\n",
      "Iteration  100 | Loss: 0.262974\n",
      "Iteration  150 | Loss: 0.263157\n",
      "Iteration  150 | Loss: 0.263157\n",
      "Iteration  199 | Loss: 0.263198\n",
      "Iteration  199 | Loss: 0.263198\n",
      "Iteration    0 | Loss: 0.263172\n",
      "Iteration    0 | Loss: 0.263172\n",
      "Iteration   50 | Loss: 0.263130\n",
      "Iteration   50 | Loss: 0.263130\n",
      "Iteration  100 | Loss: 0.262920\n",
      "Iteration  100 | Loss: 0.262920\n",
      "Iteration  150 | Loss: 0.262894\n",
      "Iteration  150 | Loss: 0.262894\n",
      "Iteration  199 | Loss: 0.263108\n",
      "Iteration  199 | Loss: 0.263108\n",
      "Iteration    0 | Loss: 0.263099\n",
      "Iteration    0 | Loss: 0.263099\n",
      "Iteration   50 | Loss: 0.263072\n",
      "Iteration   50 | Loss: 0.263072\n",
      "Iteration  100 | Loss: 0.262981\n",
      "Iteration  100 | Loss: 0.262981\n",
      "Iteration  150 | Loss: 0.262947\n",
      "Iteration  150 | Loss: 0.262947\n",
      "Iteration  199 | Loss: 0.263018\n",
      "Iteration  199 | Loss: 0.263018\n",
      "Iteration    0 | Loss: 0.263017\n",
      "Iteration    0 | Loss: 0.263017\n",
      "Iteration   50 | Loss: 0.262957\n",
      "Iteration   50 | Loss: 0.262957\n",
      "Iteration  100 | Loss: 0.263167\n",
      "Iteration  100 | Loss: 0.263167\n",
      "Iteration  150 | Loss: 0.263089\n",
      "Iteration  150 | Loss: 0.263089\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration    0 | Loss: 0.262938\n",
      "Iteration    0 | Loss: 0.262938\n",
      "Iteration   50 | Loss: 0.263004\n",
      "Iteration   50 | Loss: 0.263004\n",
      "Iteration  100 | Loss: 0.263123\n",
      "Iteration  100 | Loss: 0.263123\n",
      "Iteration  150 | Loss: 0.263222\n",
      "Iteration  150 | Loss: 0.263222\n",
      "Iteration  199 | Loss: 0.263086\n",
      "Iteration  199 | Loss: 0.263086\n",
      "Iteration    0 | Loss: 0.263106\n",
      "Iteration    0 | Loss: 0.263106\n",
      "Iteration   50 | Loss: 0.263096\n",
      "Iteration   50 | Loss: 0.263096\n",
      "Iteration  100 | Loss: 0.263041\n",
      "Iteration  100 | Loss: 0.263041\n",
      "Iteration  150 | Loss: 0.262885\n",
      "Iteration  150 | Loss: 0.262885\n",
      "Iteration  199 | Loss: 0.263045\n",
      "Iteration  199 | Loss: 0.263045\n",
      "Iteration    0 | Loss: 0.263015\n",
      "Iteration    0 | Loss: 0.263015\n",
      "Iteration   50 | Loss: 0.263017\n",
      "Iteration   50 | Loss: 0.263017\n",
      "Iteration  100 | Loss: 0.262955\n",
      "Iteration  100 | Loss: 0.262955\n",
      "Iteration  150 | Loss: 0.263235\n",
      "Iteration  150 | Loss: 0.263235\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration    0 | Loss: 0.262990\n",
      "Iteration    0 | Loss: 0.262990\n",
      "Iteration   50 | Loss: 0.263065\n",
      "Iteration   50 | Loss: 0.263065\n",
      "Iteration  100 | Loss: 0.263032\n",
      "Iteration  100 | Loss: 0.263032\n",
      "Iteration  150 | Loss: 0.262953\n",
      "Iteration  150 | Loss: 0.262953\n",
      "Iteration  199 | Loss: 0.263086\n",
      "Iteration  199 | Loss: 0.263086\n",
      "Iteration    0 | Loss: 0.263124\n",
      "Iteration    0 | Loss: 0.263124\n",
      "Iteration   50 | Loss: 0.262978\n",
      "Iteration   50 | Loss: 0.262978\n",
      "Iteration  100 | Loss: 0.262949\n",
      "Iteration  100 | Loss: 0.262949\n",
      "Iteration  150 | Loss: 0.263083\n",
      "Iteration  150 | Loss: 0.263083\n",
      "Iteration  199 | Loss: 0.263245\n",
      "Iteration  199 | Loss: 0.263245\n",
      "Iteration    0 | Loss: 0.263205\n",
      "Iteration    0 | Loss: 0.263205\n",
      "Iteration   50 | Loss: 0.262922\n",
      "Iteration   50 | Loss: 0.262922\n",
      "Iteration  100 | Loss: 0.263044\n",
      "Iteration  100 | Loss: 0.263044\n",
      "Iteration  150 | Loss: 0.263106\n",
      "Iteration  150 | Loss: 0.263106\n",
      "Iteration  199 | Loss: 0.262938\n",
      "Iteration  199 | Loss: 0.262938\n",
      "Iteration    0 | Loss: 0.262917\n",
      "Iteration    0 | Loss: 0.262917\n",
      "Iteration   50 | Loss: 0.262880\n",
      "Iteration   50 | Loss: 0.262880\n",
      "Iteration  100 | Loss: 0.262959\n",
      "Iteration  100 | Loss: 0.262959\n",
      "Iteration  150 | Loss: 0.263045\n",
      "Iteration  150 | Loss: 0.263045\n",
      "Iteration  199 | Loss: 0.262955\n",
      "Iteration  199 | Loss: 0.262955\n",
      "Iteration    0 | Loss: 0.262931\n",
      "Iteration    0 | Loss: 0.262931\n",
      "Iteration   50 | Loss: 0.263046\n",
      "Iteration   50 | Loss: 0.263046\n",
      "Iteration  100 | Loss: 0.263012\n",
      "Iteration  100 | Loss: 0.263012\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  150 | Loss: 0.262911\n",
      "Iteration  199 | Loss: 0.263021\n",
      "Iteration  199 | Loss: 0.263021\n",
      "Iteration    0 | Loss: 0.262999\n",
      "Iteration    0 | Loss: 0.262999\n",
      "Iteration   50 | Loss: 0.262891\n",
      "Iteration   50 | Loss: 0.262891\n",
      "Iteration  100 | Loss: 0.262985\n",
      "Iteration  100 | Loss: 0.262985\n",
      "Iteration  150 | Loss: 0.263031\n",
      "Iteration  150 | Loss: 0.263031\n",
      "Iteration  199 | Loss: 0.262950\n",
      "Iteration  199 | Loss: 0.262950\n",
      "Iteration    0 | Loss: 0.262973\n",
      "Iteration    0 | Loss: 0.262973\n",
      "Iteration   50 | Loss: 0.262958\n",
      "Iteration   50 | Loss: 0.262958\n",
      "Iteration  100 | Loss: 0.263067\n",
      "Iteration  100 | Loss: 0.263067\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  150 | Loss: 0.262969\n",
      "Iteration  199 | Loss: 0.263064\n",
      "Iteration  199 | Loss: 0.263064\n",
      "Iteration    0 | Loss: 0.263037\n",
      "Iteration    0 | Loss: 0.263037\n",
      "Iteration   50 | Loss: 0.263275\n",
      "Iteration   50 | Loss: 0.263275\n",
      "Iteration  100 | Loss: 0.263488\n",
      "Iteration  100 | Loss: 0.263488\n",
      "Iteration  150 | Loss: 0.262961\n",
      "Iteration  150 | Loss: 0.262961\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration    0 | Loss: 0.262919\n",
      "Iteration    0 | Loss: 0.262919\n",
      "Iteration   50 | Loss: 0.262985\n",
      "Iteration   50 | Loss: 0.262985\n",
      "Iteration  100 | Loss: 0.263008\n",
      "Iteration  100 | Loss: 0.263008\n",
      "Iteration  150 | Loss: 0.263011\n",
      "Iteration  150 | Loss: 0.263011\n",
      "Iteration  199 | Loss: 0.263125\n",
      "Iteration  199 | Loss: 0.263125\n",
      "Iteration    0 | Loss: 0.263237\n",
      "Iteration    0 | Loss: 0.263237\n",
      "Iteration   50 | Loss: 0.262952\n",
      "Iteration   50 | Loss: 0.262952\n",
      "Iteration  100 | Loss: 0.262924\n",
      "Iteration  100 | Loss: 0.262924\n",
      "Iteration  150 | Loss: 0.263183\n",
      "Iteration  150 | Loss: 0.263183\n",
      "Iteration  199 | Loss: 0.262878\n",
      "Iteration  199 | Loss: 0.262878\n",
      "Iteration    0 | Loss: 0.262907\n",
      "Iteration    0 | Loss: 0.262907\n",
      "Iteration   50 | Loss: 0.262876\n",
      "Iteration   50 | Loss: 0.262876\n",
      "Iteration  100 | Loss: 0.262877\n",
      "Iteration  100 | Loss: 0.262877\n",
      "Iteration  150 | Loss: 0.262956\n",
      "Iteration  150 | Loss: 0.262956\n",
      "Iteration  199 | Loss: 0.262993\n",
      "Iteration  199 | Loss: 0.262993\n",
      "Iteration    0 | Loss: 0.262963\n",
      "Iteration    0 | Loss: 0.262963\n",
      "Iteration   50 | Loss: 0.262951\n",
      "Iteration   50 | Loss: 0.262951\n",
      "Iteration  100 | Loss: 0.263128\n",
      "Iteration  100 | Loss: 0.263128\n",
      "Iteration  150 | Loss: 0.263106\n",
      "Iteration  150 | Loss: 0.263106\n",
      "Iteration  199 | Loss: 0.263184\n",
      "Iteration  199 | Loss: 0.263184\n",
      "Iteration    0 | Loss: 0.263139\n",
      "Iteration    0 | Loss: 0.263139\n",
      "Iteration   50 | Loss: 0.263148\n",
      "Iteration   50 | Loss: 0.263148\n",
      "Iteration  100 | Loss: 0.263229\n",
      "Iteration  100 | Loss: 0.263229\n",
      "Iteration  150 | Loss: 0.263184\n",
      "Iteration  150 | Loss: 0.263184\n",
      "Iteration  199 | Loss: 0.262938\n",
      "Iteration  199 | Loss: 0.262938\n",
      "Iteration    0 | Loss: 0.262951\n",
      "Iteration    0 | Loss: 0.262951\n",
      "Iteration   50 | Loss: 0.262890\n",
      "Iteration   50 | Loss: 0.262890\n",
      "Iteration  100 | Loss: 0.263142\n",
      "Iteration  100 | Loss: 0.263142\n",
      "Iteration  150 | Loss: 0.262887\n",
      "Iteration  150 | Loss: 0.262887\n",
      "Iteration  199 | Loss: 0.263143\n",
      "Iteration  199 | Loss: 0.263143\n",
      "Iteration    0 | Loss: 0.263148\n",
      "Iteration    0 | Loss: 0.263148\n",
      "Iteration   50 | Loss: 0.263061\n",
      "Iteration   50 | Loss: 0.263061\n",
      "Iteration  100 | Loss: 0.262872\n",
      "Iteration  100 | Loss: 0.262872\n",
      "Iteration  150 | Loss: 0.263004\n",
      "Iteration  150 | Loss: 0.263004\n",
      "Iteration  199 | Loss: 0.263060\n",
      "Iteration  199 | Loss: 0.263060\n",
      "Iteration    0 | Loss: 0.263001\n",
      "Iteration    0 | Loss: 0.263001\n",
      "Iteration   50 | Loss: 0.263018\n",
      "Iteration   50 | Loss: 0.263018\n",
      "Iteration  100 | Loss: 0.262861\n",
      "Iteration  100 | Loss: 0.262861\n",
      "Iteration  150 | Loss: 0.262918\n",
      "Iteration  150 | Loss: 0.262918\n",
      "Iteration  199 | Loss: 0.263239\n",
      "Iteration  199 | Loss: 0.263239\n",
      "Iteration    0 | Loss: 0.263180\n",
      "Iteration    0 | Loss: 0.263180\n",
      "Iteration   50 | Loss: 0.262968\n",
      "Iteration   50 | Loss: 0.262968\n",
      "Iteration  100 | Loss: 0.262987\n",
      "Iteration  100 | Loss: 0.262987\n",
      "Iteration  150 | Loss: 0.262898\n",
      "Iteration  150 | Loss: 0.262898\n",
      "Iteration  199 | Loss: 0.262912\n",
      "Iteration  199 | Loss: 0.262912\n",
      "Iteration    0 | Loss: 0.262934\n",
      "Iteration    0 | Loss: 0.262934\n",
      "Iteration   50 | Loss: 0.262950\n",
      "Iteration   50 | Loss: 0.262950\n",
      "Iteration  100 | Loss: 0.263015\n",
      "Iteration  100 | Loss: 0.263015\n",
      "Iteration  150 | Loss: 0.263116\n",
      "Iteration  150 | Loss: 0.263116\n",
      "Iteration  199 | Loss: 0.263077\n",
      "Iteration  199 | Loss: 0.263077\n",
      "Iteration    0 | Loss: 0.263068\n",
      "Iteration    0 | Loss: 0.263068\n",
      "Iteration   50 | Loss: 0.263000\n",
      "Iteration   50 | Loss: 0.263000\n",
      "Iteration  100 | Loss: 0.262896\n",
      "Iteration  100 | Loss: 0.262896\n",
      "Iteration  150 | Loss: 0.263044\n",
      "Iteration  150 | Loss: 0.263044\n",
      "Iteration  199 | Loss: 0.262999\n",
      "Iteration  199 | Loss: 0.262999\n",
      "Iteration    0 | Loss: 0.262985\n",
      "Iteration    0 | Loss: 0.262985\n",
      "Iteration   50 | Loss: 0.262941\n",
      "Iteration   50 | Loss: 0.262941\n",
      "Iteration  100 | Loss: 0.262908\n",
      "Iteration  100 | Loss: 0.262908\n",
      "Iteration  150 | Loss: 0.262922\n",
      "Iteration  150 | Loss: 0.262922\n",
      "Iteration  199 | Loss: 0.263249\n",
      "Iteration  199 | Loss: 0.263249\n",
      "Iteration    0 | Loss: 0.263221\n",
      "Iteration    0 | Loss: 0.263221\n",
      "Iteration   50 | Loss: 0.263095\n",
      "Iteration   50 | Loss: 0.263095\n",
      "Iteration  100 | Loss: 0.263080\n",
      "Iteration  100 | Loss: 0.263080\n",
      "Iteration  150 | Loss: 0.262980\n",
      "Iteration  150 | Loss: 0.262980\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration    0 | Loss: 0.262992\n",
      "Iteration    0 | Loss: 0.262992\n",
      "Iteration   50 | Loss: 0.262867\n",
      "Iteration   50 | Loss: 0.262867\n",
      "Iteration  100 | Loss: 0.263062\n",
      "Iteration  100 | Loss: 0.263062\n",
      "Iteration  150 | Loss: 0.262990\n",
      "Iteration  150 | Loss: 0.262990\n",
      "Iteration  199 | Loss: 0.262979\n",
      "Iteration  199 | Loss: 0.262979\n",
      "Iteration    0 | Loss: 0.262976\n",
      "Iteration    0 | Loss: 0.262976\n",
      "Iteration   50 | Loss: 0.262961\n",
      "Iteration   50 | Loss: 0.262961\n",
      "Iteration  100 | Loss: 0.263021\n",
      "Iteration  100 | Loss: 0.263021\n",
      "Iteration  150 | Loss: 0.262984\n",
      "Iteration  150 | Loss: 0.262984\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration  199 | Loss: 0.262997\n",
      "Iteration    0 | Loss: 0.262995\n",
      "Iteration    0 | Loss: 0.262995\n",
      "Iteration   50 | Loss: 0.263104\n",
      "Iteration   50 | Loss: 0.263104\n",
      "Iteration  100 | Loss: 0.262961\n",
      "Iteration  100 | Loss: 0.262961\n",
      "Iteration  150 | Loss: 0.262920\n",
      "Iteration  150 | Loss: 0.262920\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration  199 | Loss: 0.262922\n",
      "Iteration    0 | Loss: 0.262927\n",
      "Iteration    0 | Loss: 0.262927\n",
      "Iteration   50 | Loss: 0.262898\n",
      "Iteration   50 | Loss: 0.262898\n",
      "Iteration  100 | Loss: 0.262924\n",
      "Iteration  100 | Loss: 0.262924\n",
      "Iteration  150 | Loss: 0.263026\n",
      "Iteration  150 | Loss: 0.263026\n",
      "Iteration  199 | Loss: 0.262930\n",
      "Iteration  199 | Loss: 0.262930\n",
      "Iteration    0 | Loss: 0.262923\n",
      "Iteration    0 | Loss: 0.262923\n",
      "Iteration   50 | Loss: 0.263122\n",
      "Iteration   50 | Loss: 0.263122\n",
      "Iteration  100 | Loss: 0.263134\n",
      "Iteration  100 | Loss: 0.263134\n",
      "Iteration  150 | Loss: 0.263082\n",
      "Iteration  150 | Loss: 0.263082\n",
      "Iteration  199 | Loss: 0.262947\n",
      "Iteration  199 | Loss: 0.262947\n",
      "Iteration    0 | Loss: 0.262940\n",
      "Iteration    0 | Loss: 0.262940\n",
      "Iteration   50 | Loss: 0.262990\n",
      "Iteration   50 | Loss: 0.262990\n",
      "Iteration  100 | Loss: 0.262898\n",
      "Iteration  100 | Loss: 0.262898\n",
      "Iteration  150 | Loss: 0.263048\n",
      "Iteration  150 | Loss: 0.263048\n",
      "Iteration  199 | Loss: 0.262936\n",
      "Iteration  199 | Loss: 0.262936\n",
      "Iteration    0 | Loss: 0.262943\n",
      "Iteration    0 | Loss: 0.262943\n",
      "Iteration   50 | Loss: 0.263024\n",
      "Iteration   50 | Loss: 0.263024\n",
      "Iteration  100 | Loss: 0.263133\n",
      "Iteration  100 | Loss: 0.263133\n",
      "Iteration  150 | Loss: 0.263134\n",
      "Iteration  150 | Loss: 0.263134\n",
      "Iteration  199 | Loss: 0.263065\n",
      "Iteration  199 | Loss: 0.263065\n",
      "Iteration    0 | Loss: 0.263204\n",
      "Iteration    0 | Loss: 0.263204\n",
      "Iteration   50 | Loss: 0.263082\n",
      "Iteration   50 | Loss: 0.263082\n",
      "Iteration  100 | Loss: 0.263173\n",
      "Iteration  100 | Loss: 0.263173\n",
      "Iteration  150 | Loss: 0.262894\n",
      "Iteration  150 | Loss: 0.262894\n",
      "Iteration  199 | Loss: 0.262896\n",
      "Iteration  199 | Loss: 0.262896\n",
      "Iteration    0 | Loss: 0.262904\n",
      "Iteration    0 | Loss: 0.262904\n",
      "Iteration   50 | Loss: 0.262936\n",
      "Iteration   50 | Loss: 0.262936\n",
      "Iteration  100 | Loss: 0.263155\n",
      "Iteration  100 | Loss: 0.263155\n",
      "Iteration  150 | Loss: 0.263015\n",
      "Iteration  150 | Loss: 0.263015\n",
      "Iteration  199 | Loss: 0.262931\n",
      "Iteration  199 | Loss: 0.262931\n",
      "Iteration    0 | Loss: 0.262971\n",
      "Iteration    0 | Loss: 0.262971\n",
      "Iteration   50 | Loss: 0.262913\n",
      "Iteration   50 | Loss: 0.262913\n",
      "Iteration  100 | Loss: 0.262950\n",
      "Iteration  100 | Loss: 0.262950\n",
      "Iteration  150 | Loss: 0.262962\n",
      "Iteration  150 | Loss: 0.262962\n",
      "Iteration  199 | Loss: 0.262880\n",
      "Iteration  199 | Loss: 0.262880\n",
      "Iteration    0 | Loss: 0.262875\n",
      "Iteration    0 | Loss: 0.262875\n",
      "Iteration   50 | Loss: 0.262909\n",
      "Iteration   50 | Loss: 0.262909\n",
      "Iteration  100 | Loss: 0.262891\n",
      "Iteration  100 | Loss: 0.262891\n",
      "Iteration  150 | Loss: 0.262874\n",
      "Iteration  150 | Loss: 0.262874\n",
      "Iteration  199 | Loss: 0.262967\n",
      "Iteration  199 | Loss: 0.262967\n",
      "Iteration    0 | Loss: 0.262988\n",
      "Iteration    0 | Loss: 0.262988\n",
      "Iteration   50 | Loss: 0.263049\n",
      "Iteration   50 | Loss: 0.263049\n",
      "Iteration  100 | Loss: 0.262921\n",
      "Iteration  100 | Loss: 0.262921\n",
      "Iteration  150 | Loss: 0.262946\n",
      "Iteration  150 | Loss: 0.262946\n",
      "Iteration  199 | Loss: 0.262981\n",
      "Iteration  199 | Loss: 0.262981\n"
     ]
    }
   ],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        if schedule == \"step\":\n",
    "            learning_rate = initial_lr * (0.9 ** (i / 100))\n",
    "        \n",
    "        elif schedule == \"exponential\":\n",
    "            learning_rate = initial_lr * np.exp(-decay_constant * i)\n",
    "        \n",
    "        elif schedule == \"inverse\":\n",
    "            learning_rate = initial_lr / (1 + decay_constant * i)\n",
    "\n",
    "        y_pred = predict(X, w, b)\n",
    "\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_pred)\n",
    "\n",
    "        w = w - (learning_rate * grad_w)\n",
    "        b = b - (learning_rate * grad_b)\n",
    "\n",
    "        loss = compute_mse(y, y_pred)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "      \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step decay:\n",
      "Final loss: 0.262860\n",
      "Initial loss: 45.905040\n",
      "Loss decreased by: 45.642181\n",
      "\n",
      "Exponential decay:\n",
      "Final loss: 0.262855\n",
      "Initial loss: 45.871055\n",
      "Loss decreased by: 45.608200\n",
      "\n",
      "Inverse time decay:\n",
      "Final loss: 0.262855\n",
      "Initial loss: 45.896969\n",
      "Loss decreased by: 45.634113\n"
     ]
    }
   ],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(f\"Final loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"Initial loss: {loss_history[0]:.6f}\")\n",
    "print(f\"Loss decreased by: {loss_history[0] - loss_history[-1]:.6f}\\n\")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(f\"Final loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"Initial loss: {loss_history[0]:.6f}\")\n",
    "print(f\"Loss decreased by: {loss_history[0] - loss_history[-1]:.6f}\\n\")\n",
    "print(\"Inverse time decay:\")\n",
    "\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(f\"Final loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"Initial loss: {loss_history[0]:.6f}\")\n",
    "print(f\"Loss decreased by: {loss_history[0] - loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mse = compute_mse(y_true, y_pred)\n",
    "    \n",
    "    l2_penalty = reg_lambda * np.sum(w ** 2)\n",
    "    \n",
    "    ridge_loss = mse + l2_penalty\n",
    "    \n",
    "    return ridge_loss\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    n = len(y)\n",
    "\n",
    "    gradient_w = (2 / n) * X.T.dot(y_pred - y)\n",
    "    gradient_b = (2 / n) * np.sum(y_pred - y)\n",
    "\n",
    "    gradient_w = gradient_w + 2 * reg_lambda * w\n",
    "\n",
    "    return gradient_w, gradient_b\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        y_pred = predict(X, w, b)\n",
    "\n",
    "        loss = calculate_ridge_loss(y, y_pred, w, reg_lambda)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        grad_w, grad_b = calculate_ridge_gradients(X, y, y_pred, w, reg_lambda)\n",
    "\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "\n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.818693\n",
      "Iteration  100 | Loss: 2.509656\n",
      "Iteration  200 | Loss: 1.958884\n",
      "Iteration  300 | Loss: 1.951144\n",
      "Iteration  400 | Loss: 1.951022\n"
     ]
    }
   ],
   "source": [
    "_, _, _ =train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
